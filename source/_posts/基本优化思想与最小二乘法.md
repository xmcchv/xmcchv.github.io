---
title: 基本优化思想与最小二乘法
date: 2021-08-08 17:23:59
mathjax: True
tags:
- pytorch
---
## 基本优化思想与最小二乘法
在pytorch中最核心的基础数学工具就是梯度计算工具，也就是自动微分模块。
所谓优化思想，指的是利用数学工具求解复杂问题的基本思想，我们往往会先给出待解决问题的数值评估指标，并在此基础之上构建方程，采用数学工具，不断优化评估指标结果，以达到最优结果。


```python
import numpy as np
import torch
```

### 一、简单线性回归的机器学习建模思路
1. 回顾简单线性回归建模问题



```python
import matplotlib as mpl
import matplotlib.pyplot as plt
```


```python
A=torch.arange(1,5).reshape(2,2).float()
A
```




    tensor([[1., 2.],
            [3., 4.]])




```python
plt.plot(A[:,0],A[:,1],'o')
```




    [<matplotlib.lines.Line2D at 0x1f7eff7f160>]




    
![png](/images/output_5_1.png)
    


2. 转化为优化问题
上述问题除了可以用矩阵方法求解以外，还可以将其转化为最优化问题，然后通过求解最优化问题的方法对其进行求解。最优化问题的转化分为两步。其一是确定优化数值指标，其二是确定优化目标函数。
如果我们希望通过一条直线拟合二维平面空间上分布的点，最核心的目标就是希望方程的预测值和真实值相差较小。假设真实的y值用y表示，预测值用$\widehat{y}$表示，带入a、b参数，择优数值表示如下：





$x^{(i)}$|$y^{(i)}$|$\widehat{y}^{(i)}$
:-|:-|:-
1|2|a+b
3|4|3a+b




$\widehat{y}$ 表示对应预测值



$$\widehat{y}_{1} = 1 * a + b = a + b$$


$$\widehat{y}_{2} = 3 * a + b = 3a + b$$


因此

$$(y_{1}-\widehat{y}_{1})^{2}+(y_{2}-\widehat{y}_{2})^{2}=(2-a-b)^{2}+(4-3a-b)^{2}$$



上式就是两个点的预测值和真实值的差值平方和，也就是误差平方和——SSE(Sum of the Squard Errors)


> 此处我们只带入(1,2)和(3,4)两个点来计算SSE，也就是带入了两条数据来训练y=ax+b这个模型。

当a,b取何值时，SSE取值最小?SSE方程就是我们优化的目标方程（求最小值），因此上述方程也被成为目标函数，同时，SSE代表着真实值和预测值之间的差值(误差平方和)，因此也被成为损失函数(预测值距真实值的损失)。

> 换而言之，当SSE取值最小时，a,b的取值就是最终线性回归方程的系数取值

> 目标函数和损失函数并不完全等价，但大多数目标函数都由损失函数构成

3. 最优化问题的求解方法
在机器学习领域，大多数优化问题都是围绕目标函数(或者损失函数)进行求解。在上述问题中，我们需要围绕SSE求最小值，SSE是一个关于a和b的二元函数，要求最小值，需要最优化方法，选择优化方法并执行相应计算。
- 图形展示目标函数
  使用Python中matplotlib包和Axes3D函数进行三维图像绘制


```python
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
```


```python
x=np.arange(-1,3,0.05)
y=np.arange(-1,3,0.05)
a,b=np.meshgrid(x,y)
SSE=(2-a-b)**2+(4-3*a-b)**2
```


```python
fig=plt.figure()
ax=plt.axes(projection='3d')

ax.plot_surface(a,b,SSE,cmap='rainbow')
ax.contour(a,b,SSE,zdir='z',offset=0,cmap="rainbow") # 生成z方向的投影，投到x-y平面
plt.show()
```


    
![png](images/output_9_0.png)
    


- 函数的凹凸性
绘制$y=x^{2}$图像


```python
x=np.arange(-10,10,0.1)
y=x**2
plt.plot(x,y,'-')
plt.show()
```


    
![png](/images/output_11_0.png)
    



```python
x=np.arange(-5,5,0.5)
y=x**2+2*x+1
plt.plot(x,y,'-')
plt.show()
```


    
![png](/images/output_12_0.png)
    


- SSE最小值
对于简单线性回归的损失函数，SSE是凸函数，因此对于SSE(a,b)=(2-a-b)^2+(4-3a-b)^2而言，最小值点就是a,b两个参数求偏导等于0的点。



$$\frac{\partial SSE(a,b)}{\partial a}=20a+8b-28=0$$




$$\frac{\partial SSE(a,b)}{\partial b}=8a+4b-12=0$$




所以a=1,b=1，因此y=x+1
利用偏导等于0得出的方程组求解线性回归方程参数，就是最小二乘法求解过程。


4. 机器学习建模一般流程
- Step 1：提出基本模型
  本节中，我们使用的直线(y=ax+b)去拟合二维平面空间中的点，这条直线就是基本模型，不同的模型能够使用不同的场景，在提出模型时，我们往往会预设一些影响模型结构或者实际判别性能的参数，如简单线性回归中a和b；
- Step 2：确定损失函数和目标函数
  围绕建模的目标，需要设置合理损失函数，并在此基础上设置目标函数，很多情况下，这二者是相同的，这里需要注意，损失函数不是模型，而是模型参数所组成的一个函数。
- Step 3：根据目标函数特性，选择优化方法，求解目标函数
  目标函数既承载了我们优化的目标(让预测值和真实值尽可能接近)，同时也是包含了模型参数的函数，因此，完成建模需要确定参数、优化结果需要预测值尽可能接近真实值这两方面需求就统一到了求解目标函数最小值的过程中了，也就是说，当我们围绕目标函数求解最小值时，也就完成了模型参数的求解。不同类型、不同性质的函数会影响优化方法的选择。在简单线性回归中，由于目标函数是凸函数，我们选取偏导数取值为0的点就是最小值点，进而使用最小二乘法完成a,b的计算，就是通过函数本身的性质进行最优化方法的选取。

### 二、第一个优化算法：最小二乘法
1. 最小二乘法的代数表示方法
    从更加严格的数学角度出发，最小二乘法有两种表示形式，分别是代数法表示和矩阵表示，我们先看最小二乘法的代数表示方法。首先，假设多元线性方程有如下形式


    $$f(x)=w_{1}x_{1}+w_{2}x_{2}+\dots+w_{d}x_{d}+b$$


令$ w=(w_{1},w_{2},\dots w_{d}),x=(x_{1},x_{2},\dots x_{d}) $则上式可写为


$$f(x)=w^{T}x+b$$


> 在机器学习领域，线性回归自变量系数一般命名为w，weight的简写，指自变量的权重

多元线性回归的最小二乘法的代数表示较为复杂，此处先考虑简单线性回归的最小二乘法表示形式，在简单线性回归中，w只包含一个分量，x也只包含一个分量，我们令此时的$ x_{i} $就是对应的自变量的取值。
优化目标可写为


$$SSE=\sum^{m}_{i=1}(f(x_{i}-y_{i})^{2}=E(w,b)$$


通过偏导为0求得最终结果的最小二乘法求解过程为：



$$\frac{\partial SSE(w,b)}{\partial w}=2(w\sum^{m}_{i=1}x_{i}^{2}-\sum^{m}_{i=1}(y_{i}-b)x_{i})=0$$



$$\frac{\partial SSE(w,b)}{\partial b}=2(mb-\sum^{m}_{i=1}(y_{i}-wx_{i}))=0$$
