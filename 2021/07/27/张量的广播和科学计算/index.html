<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="云间之龙">
    
    <title>
        
            张量的广播和科学计算 |
        
        云间小栈
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="/images/avatar.jpg">
    
<link rel="stylesheet" href="/css/font-awesome.min.css">

    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"xmcchv.github.io","root":"/","language":"zh-CN","path":"search.xml"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":false,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/avatar.jpg","favicon":"/images/avatar.jpg","article_img_align":"left","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":true,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"凡心所向，素履所往；生如逆旅，一苇以航"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":true}}},"local_search":{"enable":true,"preload":true},"code_copy":{"enable":true,"style":"default"},"pjax":{"enable":false},"lazyload":{"enable":true},"version":"3.4.5"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
  </script>
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            <header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                云间小栈
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               target="_blank" rel="noopener" href="https://xmcchv.vercel.app/zh-CN/"
                            >
                                下载站
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       target="_blank" rel="noopener" href="https://xmcchv.vercel.app/zh-CN/">下载站</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="article-content-container">

        <div class="article-title">
            <span class="title-hover-animation">张量的广播和科学计算</span>
        </div>

        
            <div class="article-header">
                <div class="avatar">
                    <img src="/images/avatar.jpg">
                </div>
                <div class="info">
                    <div class="author">
                        <span class="name">云间之龙</span>
                        
                    </div>
                    <div class="meta-info">
                        <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fas fa-edit"></i>&nbsp;
        <span class="pc">2021-07-27 21:46:44</span>
        <span class="mobile">2021-07-27 21:46</span>
    </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/pytorch/">pytorch</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                    </div>
                </div>
            </div>
        

        <div class="article-content markdown-body">
            <h2 id="张量的广播和科学运算"><a href="#张量的广播和科学运算" class="headerlink" title="张量的广播和科学运算"></a>张量的广播和科学运算</h2><p>作为pytorch中执行深度学习的基本数据类型，张量也拥有非常多的数学与运算函数和方法，以及对应的一系列计算规则。在pytorch中，能够作用于tensor的运算被统一称作为算子</p>
<ul>
<li>数学运算的分类</li>
</ul>
<ol>
<li>逐点运算（pointwise ops）对tensor中每个元素执行相同的运算操作</li>
<li>规约运算（reduction ops）对某一张两进行操作得出某种总结值</li>
<li>比较运算（comparison ops）对多个张量进行比较运算的相关方法</li>
<li>谱运算（spectral ops）涉及信号处理傅里叶变化的操作</li>
<li>BLAS和LAPACK运算 基础线性代数程序集（Basic Linear Algeria Subprograms）和线性代数包（Linear Algeria Package）中定义的、主要用于现行代数科学计算的函数和方法</li>
<li>其他运算 其他未被归类的数学运算</li>
</ol>
<h3 id="一、张量的广播-broadcast-特性"><a href="#一、张量的广播-broadcast-特性" class="headerlink" title="一、张量的广播(broadcast)特性"></a>一、张量的广播(broadcast)特性</h3><p>允许不同形状的张量之间进行计算</p>
<h4 id="1-相同形状的张量计算"><a href="#1-相同形状的张量计算" class="headerlink" title="1.相同形状的张量计算"></a>1.相同形状的张量计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">3</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1+t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1**t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 1, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1/t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([nan, 1., 1.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]+[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>[0, 1, 2, 0, 1, 2]
</code></pre>
<h4 id="2-不同形状的张量计算"><a href="#2-不同形状的张量计算" class="headerlink" title="2. 不同形状的张量计算"></a>2. 不同形状的张量计算</h4><p>广播的特性是在不同形状的张量进行计算时，一个或多个张量通过隐式转化为相同形状的两个张量，但是，并非任何两个不同形状的张量都可以通过广播特性进行计算</p>
<h5 id="2-1-标量和任意形状的张量"><a href="#2-1-标量和任意形状的张量" class="headerlink" title="2.1 标量和任意形状的张量"></a>2.1 标量和任意形状的张量</h5><p>标量可以和任意形状的张量进行计算，计算过程就是标量和张量的每一个元素进行计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1+<span class="number">1</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*<span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(t1+<span class="number">1</span>)**<span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 4, 9])
</code></pre>
<h5 id="2-2-相同维度、不同形状的计算"><a href="#2-2-相同维度、不同形状的计算" class="headerlink" title="2.2 相同维度、不同形状的计算"></a>2.2 相同维度、不同形状的计算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.zeros(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t21=torch.ones(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t21</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t21+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre>
<p>此处的广播相当于将t21的形状（1,4）拓展成了t2（3,4）<br>即复制了第一行三次，然后二者相加<br>也可理解为 t21的第一行和t2的三行分别进行相加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t22=torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t22</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t22.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<p>广播规则：</p>
<ol>
<li>两个张量在一个维度一致，另一个维度上其中一个张量为1</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.ones(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 1])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre>
<ol start="2">
<li>t3的形状为（3,1），t31的形状为（1,3），二者的形状在两个分量上均不同，但都有1存在，因此也可以广播</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t31=torch.ones(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t31+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])
</code></pre>
<p>三维张量的广播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t31=torch.ones(<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1.],
         [1.],
         [1.],
         [1.]],

        [[1.],
         [1.],
         [1.],
         [1.]],

        [[1.],
         [1.],
         [1.],
         [1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3+t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t32=torch.ones(<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">t32</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t32+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t32+t31 <span class="comment"># 一维度一致，另两个维度不同，但都有 1</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]],

        [[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]],

        [[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]]])
</code></pre>
<h5 id="2-3不同维度的张量计算过程中的广播"><a href="#2-3不同维度的张量计算过程中的广播" class="headerlink" title="2.3不同维度的张量计算过程中的广播"></a>2.3不同维度的张量计算过程中的广播</h5><p>低维张量升维，只需要将更高维度上填充1即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">4</span>).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1],
        [2, 3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0, 1],
         [2, 3]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[0, 1],
          [2, 3]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t3+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.],
        [1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t3+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
</code></pre>
<h3 id="二、逐点运算"><a href="#二、逐点运算" class="headerlink" title="二、逐点运算"></a>二、逐点运算</h3><h4 id="tensor基本数学运算"><a href="#tensor基本数学运算" class="headerlink" title="tensor基本数学运算"></a>tensor基本数学运算</h4><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.add(t1,t2)</td>
<td align="left">t1+t2</td>
</tr>
<tr>
<td align="left">torch.subtract(t1,t2)</td>
<td align="left">t1-t2</td>
</tr>
<tr>
<td align="left">torch.multiply(t1,t2)</td>
<td align="left">t1*t2</td>
</tr>
<tr>
<td align="left">torch.divide(t1,t2)</td>
<td align="left">t1&#x2F;t2</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([4, 6])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.multiply(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([3, 8])
</code></pre>
<h4 id="tensor数值调整函数"><a href="#tensor数值调整函数" class="headerlink" title="tensor数值调整函数"></a>tensor数值调整函数</h4><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.abs()</td>
<td align="left">返回绝对值</td>
</tr>
<tr>
<td align="left">torch.ceil()</td>
<td align="left">向上取整</td>
</tr>
<tr>
<td align="left">torch.floor()</td>
<td align="left">向下取整</td>
</tr>
<tr>
<td align="left">torch.round()</td>
<td align="left">四舍五入</td>
</tr>
<tr>
<td align="left">torch.neg()</td>
<td align="left">取反</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">5</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0.4222, -2.3513, -2.0652, -0.3838, -0.2116])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">round</span>(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0., -2., -2., -0., -0.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.abs_()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.4222, 2.3513, 2.0652, 0.3838, 0.2116])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.neg_()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0.4222, -2.3513, -2.0652, -0.3838, -0.2116])
</code></pre>
<table>
<thead>
<tr>
<th align="left">数学运算函数</th>
<th align="left">数学公式</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">幂运算</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">torch.exp(t)</td>
<td align="left">$ y_{i} &#x3D; e^{x_{i}} $</td>
<td align="left">返回以e为底、t中元素为幂的张量</td>
</tr>
<tr>
<td align="left">torch.expm1(t)</td>
<td align="left">$ y_{i} &#x3D; e^{x_{i}} $ - 1</td>
<td align="left">对张量中的所有元素计算exp（x） - 1</td>
</tr>
<tr>
<td align="left">torch.exp2(t)</td>
<td align="left">$ y_{i} &#x3D; 2^{x_{i}} $</td>
<td align="left">逐个元素计算2的t次方。</td>
</tr>
<tr>
<td align="left">torch.pow(t,n)</td>
<td align="left">$\text{out}_i &#x3D; x_i ^ \text{exponent} $</td>
<td align="left">返回t的n次幂</td>
</tr>
<tr>
<td align="left">torch.sqrt(t)</td>
<td align="left">$ \text{out}{i} &#x3D; \sqrt{\text{input}{i}} $</td>
<td align="left">返回t的平方根</td>
</tr>
<tr>
<td align="left">torch.square(t)</td>
<td align="left">$ \text{out}_i &#x3D; x_i ^ \text{2} $</td>
<td align="left">返回输入的元素平方。</td>
</tr>
<tr>
<td align="left">对数运算</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">torch.log10(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{10} (x_{i}) $</td>
<td align="left">返回以10为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{e} (x_{i}) $</td>
<td align="left">返回以e为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log2(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{2} (x_{i}) $</td>
<td align="left">返回以2为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log1p(t)</td>
<td align="left">$ y_i &#x3D; \log_{e} (x_i $ + 1)</td>
<td align="left">返回一个加自然对数的输入数组。</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(torch.tensor(<span class="number">2</span>),<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(4)
</code></pre>
<ul>
<li>tensor的大多数科学运算具有一定的静态性<br>静态性就是对输入的张量类型有明确的要求，例如部分函数只能输入浮点型张量，而不能输入整形张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t.dtype</span><br></pre></td></tr></table></figure>




<pre><code>torch.int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.7183,  7.3891, 20.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=t.<span class="built_in">float</span>()</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 2., 3.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.7183,  7.3891, 20.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.expm1(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1.7183,  6.3891, 19.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(t,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 4, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(t,<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.4142, 1.7321])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(torch.log(t1))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 2., 3.])
</code></pre>
<ul>
<li>排序运算：sort<br>在pytorch中，sort函数将同时返回排序结果和对应的索引值的排列</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.tensor([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([1, 2, 3, 4]),
indices=tensor([0, 2, 1, 3]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t,descending=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([4, 3, 2, 1]),
indices=tensor([3, 1, 2, 0]))
</code></pre>
<h3 id="三、规约运算"><a href="#三、规约运算" class="headerlink" title="三、规约运算"></a>三、规约运算</h3><p>规约运算指针对某张量进行某种总结，最后得出一个具体总结值的函数。主要包含数据科学领域内的诸多统计分析函数，如均值、极值、方差、中位数函数等等。</p>
<table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.mean(t)</td>
<td align="left">返回张量均值</td>
</tr>
<tr>
<td align="left">torch.var(t)</td>
<td align="left">返回张量方差</td>
</tr>
<tr>
<td align="left">torch.std(t)</td>
<td align="left">返回张量标准差</td>
</tr>
<tr>
<td align="left">torch.var_mean(t)</td>
<td align="left">返回张量方差和均值</td>
</tr>
<tr>
<td align="left">torch.std_mean(t)</td>
<td align="left">返回张量标准差和均值</td>
</tr>
<tr>
<td align="left">torch.max(t)</td>
<td align="left">返回张量最大值</td>
</tr>
<tr>
<td align="left">torch.argmax(t)</td>
<td align="left">返回张量最大值索引</td>
</tr>
<tr>
<td align="left">torch.min(t)</td>
<td align="left">返回张量最小值</td>
</tr>
<tr>
<td align="left">torch.argmin(t)</td>
<td align="left">返回张量最小值索引</td>
</tr>
<tr>
<td align="left">torch.median(t)</td>
<td align="left">返回张量中位数</td>
</tr>
<tr>
<td align="left">torch.sum(t)</td>
<td align="left">返回张量求和结果</td>
</tr>
<tr>
<td align="left">torch.logsumexp(t)</td>
<td align="left">返回张量各元素求和结果，适用于数据量较小的情况</td>
</tr>
<tr>
<td align="left">torch.prod(t)</td>
<td align="left">返回张量累乘结果</td>
</tr>
<tr>
<td align="left">torch.dist(t1, t2)</td>
<td align="left">计算两个张量的闵式距离，可使用不同范式</td>
</tr>
<tr>
<td align="left">torch.topk(t)</td>
<td align="left">返回t中最大的k个值对应的指标</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.std_mean(t)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor(3.0277), tensor(4.5000))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.prod(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>




<pre><code>tensor(24)
</code></pre>
<ul>
<li>dist计算距离<br>dist函数可计算闵式距离(闵可夫斯基距离)，通过输入不同的p值，可以计算多种类型的距离，如欧式距离，街道距离等。闵可夫斯基距离公式表示如下：</li>
</ul>
<p>$$D(x,y)&#x3D;(\sum_{u&#x3D;1}^{n}{|x_{u}-y_{u}|}^{p} )^{1&#x2F;p}$$</p>
<p>p取值为2时，计算欧式距离</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>])</span><br><span class="line">t2=torch.tensor([<span class="number">3.0</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dist(t1,t2,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2.8284)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sqrt(torch.tensor(<span class="number">8.0</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2.8284)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dist(t1,t2,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(4.)
</code></pre>
<ul>
<li>规约运算的维度<br>由于规约运算是一个序列返回一个结果，因此若是针对高维张良，则可能指定某维度进行计算。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).<span class="built_in">float</span>().reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t2,dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([12., 15., 18., 21.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t2,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 6., 22., 38.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">24</span>).<span class="built_in">float</span>().reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]],

        [[12., 13., 14., 15.],
         [16., 17., 18., 19.],
         [20., 21., 22., 23.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[12., 14., 16., 18.],
        [20., 22., 24., 26.],
        [28., 30., 32., 34.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[12., 15., 18., 21.],
        [48., 51., 54., 57.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 6., 22., 38.],
        [54., 70., 86.]])
</code></pre>
<ul>
<li>二维张量的排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t22=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t22</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.1265,  0.2807,  0.7180,  0.8772],
        [ 1.9022,  0.3509, -0.4520, -2.3835],
        [-0.5028, -0.8871, -2.2325,  0.7211]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t22) <span class="comment"># 默认情况下，按照行进行升序排序</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([[-0.1265,  0.2807,  0.7180,  0.8772],
        [-2.3835, -0.4520,  0.3509,  1.9022],
        [-2.2325, -0.8871, -0.5028,  0.7211]]),
indices=tensor([[0, 1, 2, 3],
        [3, 2, 1, 0],
        [2, 1, 0, 3]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t22,dim=<span class="number">1</span>,descending=<span class="literal">True</span>) <span class="comment"># 按列降序排序</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([[ 0.8772,  0.7180,  0.2807, -0.1265],
        [ 1.9022,  0.3509, -0.4520, -2.3835],
        [ 0.7211, -0.5028, -0.8871, -2.2325]]),
indices=tensor([[3, 2, 1, 0],
        [0, 1, 2, 3],
        [3, 0, 1, 2]]))
</code></pre>
<h3 id="四、比较运算"><a href="#四、比较运算" class="headerlink" title="四、比较运算"></a>四、比较运算</h3><p>比较运算是一类较为简单的运算类型，和python原生的布尔运算类似，常用于不同张量之间的逻辑运算，最终返回逻辑运算结果（逻辑类型张量）基本比较运算函数如下所示：</p>
<table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.eq(t1,t2)</td>
<td align="left">等效&#x3D;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.equal(t1,t2)</td>
<td align="left">判断两个张量是否是相同的张量</td>
</tr>
<tr>
<td align="left">torch.gt(t1,t2)</td>
<td align="left">等效&gt;</td>
</tr>
<tr>
<td align="left">torch.lt(t1,t2)</td>
<td align="left">等效于&lt;</td>
</tr>
<tr>
<td align="left">torch.ge(t1,t2)</td>
<td align="left">等效&gt;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.le(t1,t2)</td>
<td align="left">等效&lt;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.ne(t1,t2)</td>
<td align="left">等效!&#x3D;</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1.0</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">t2=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">t1==t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.equal(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>False
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.lt(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([False, False,  True])
</code></pre>

        </div>

        

        
            <ul class="post-tags-box">
                
                    <li class="tag-item">
                        <a href="/tags/pytorch/">#pytorch</a>&nbsp;
                    </li>
                
            </ul>
        

        
            <div class="article-nav">
                
                    <div class="article-prev">
                        <a class="prev"
                           rel="prev"
                           href="/2021/07/31/%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E8%BF%90%E7%AE%97/"
                        >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                            <span class="title flex-center">
                                <span class="post-nav-title-item">张量的线性代数运算</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                        </a>
                    </div>
                
                
                    <div class="article-next">
                        <a class="next"
                           rel="next"
                           href="/2021/07/24/tensor%E5%9F%BA%E7%A1%80/"
                        >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">tensor基础</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                            <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                        </a>
                    </div>
                
            </div>
        

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
              <span>2021</span>
              -
            
            2025&nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;<a href="/">云间之龙</a>
        </div>
        
        <div class="theme-info info-item">
            由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.4.5</a>
        </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item page-aside-toggle">
                <i class="fas fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-expand-width flex-center">
            <i class="fas fa-arrows-alt-h"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
    </ul>
</div>

    </div>

    
        <aside class="page-aside">
            <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD%E5%92%8C%E7%A7%91%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="nav-text">张量的广播和科学运算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD-broadcast-%E7%89%B9%E6%80%A7"><span class="nav-text">一、张量的广播(broadcast)特性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%9B%B8%E5%90%8C%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BC%A0%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="nav-text">1.相同形状的张量计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E4%B8%8D%E5%90%8C%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BC%A0%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="nav-text">2. 不同形状的张量计算</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-%E6%A0%87%E9%87%8F%E5%92%8C%E4%BB%BB%E6%84%8F%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="nav-text">2.1 标量和任意形状的张量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-%E7%9B%B8%E5%90%8C%E7%BB%B4%E5%BA%A6%E3%80%81%E4%B8%8D%E5%90%8C%E5%BD%A2%E7%8A%B6%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">2.2 相同维度、不同形状的计算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3%E4%B8%8D%E5%90%8C%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%BC%A0%E9%87%8F%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD"><span class="nav-text">2.3不同维度的张量计算过程中的广播</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E9%80%90%E7%82%B9%E8%BF%90%E7%AE%97"><span class="nav-text">二、逐点运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor%E5%9F%BA%E6%9C%AC%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="nav-text">tensor基本数学运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor%E6%95%B0%E5%80%BC%E8%B0%83%E6%95%B4%E5%87%BD%E6%95%B0"><span class="nav-text">tensor数值调整函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E8%A7%84%E7%BA%A6%E8%BF%90%E7%AE%97"><span class="nav-text">三、规约运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%AF%94%E8%BE%83%E8%BF%90%E7%AE%97"><span class="nav-text">四、比较运算</span></a></li></ol></li></ol>
    </div>
</div>
        </aside>
    

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>



    
<script src="/js/local-search.js"></script>




    
<script src="/js/code-copy.js"></script>




    
<script src="/js/lazyload.js"></script>



<div class="post-scripts">
    
        
<script src="/js/left-side-toggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/toc.js"></script>

    
</div>



</body>
</html>
