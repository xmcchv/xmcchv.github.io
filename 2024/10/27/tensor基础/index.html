<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Keep Team">
    
    <title>
        
        Keep Theme
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/logo.svg">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"xmcchv.github.io","root":"/","language":"zh-CN","path":"search.xml"}
    KEEP.theme_config = {"toc":{"enable":false,"number":false,"expand_all":false,"init_open":true,"layout":"right"},"style":{"primary_color":"#0066cc","logo":"/images/logo.svg","favicon":"/images/logo.svg","avatar":"/images/avatar.svg","first_screen":{"enable":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving.","hitokoto":false},"scroll":{"progress_bar":false,"percent":false}},"local_search":{"enable":false,"preload":false},"code_block":{"tools":{"enable":false,"style":"default"},"highlight_theme":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.21"},"waline":{"server_url":null,"reaction":false,"version":2},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"wordcount":false,"min2read":false},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":false,"share":false,"reward":{"enable":false,"img_link":null,"text":null}},"website_count":{"busuanzi_count":{"enable":false,"site_uv":false,"site_pv":false,"page_pv":false}},"version":"3.8.6"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container border-box">

    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.svg">
                </a>
            
            <a class="site-name border-box" href="/">
               Keep Theme
            </a>
        </div>

        <div class="right border-box">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">

                

                    <div class="fade-in-down-animation">
    <div class="post-page-container border-box">

        <div class="article-content-container border-box">

            

            <div class="article-content-bottom border-box">
                
                    <div class="article-title">
                        
                    </div>
                

                
                    <div class="article-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.svg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author">
                                <span class="name">Keep Team</span>
                                
                                    <span class="author-label">Lv3</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="article-meta-info-container border-box post">
    <div class="article-meta-info border-box">
        


        
            <span class="meta-info-item article-create-date">
                <i class="icon fa-solid fa-calendar-check"></i>&nbsp;
                <span class="pc">2024-10-27 00:42:49</span>
                <span class="mobile">2024-10-27 00:42</span>
            </span>

            <span class="meta-info-item article-update-date">
                <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                <span class="pc" data-updated="Sun Oct 27 2024 00:42:49 GMT+0800">2024-10-27 00:42:49</span>
            </span>
        

        

        

        
        
        
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="article-content keep-markdown-body">
                    

                    <h2 id="lt-lt-lt-lt-lt-lt-lt-HEAD"><a href="#lt-lt-lt-lt-lt-lt-lt-HEAD" class="headerlink" title="&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD"></a>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</h2><p>title: tensor基础<br>author: xmcchv<br>date: 2021-07-24 17:39:29<br>tags:</p>
<ul>
<li>pytorch</li>
</ul>
<hr>
<h2 id="张量的创建与常用方法"><a href="#张量的创建与常用方法" class="headerlink" title="张量的创建与常用方法"></a>张量的创建与常用方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a1=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">a1</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3],
       [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.tensor([a1,a1])</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[1, 2, 3],
         [4, 5, 6]]], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.ndim</span><br></pre></td></tr></table></figure>




<pre><code>3
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(t3)</span><br></pre></td></tr></table></figure>




<pre><code>2
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.flatten()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转化为两行，一列的张量</span></span><br><span class="line">t1.reshape(<span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1],
        [2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.reshape(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape([<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1],
        [2],
        [3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.reshape(<span class="number">12</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.reshape(<span class="number">2</span>,<span class="number">6</span>) <span class="comment"># 转换为两行六列的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3, 4, 5, 6],
        [1, 2, 3, 4, 5, 6]], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(<span class="number">5</span>)  <span class="comment"># 单位矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1</span><br><span class="line">torch.diag(t1) <span class="comment"># 不能使用list直接创建对角矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 0],
        [0, 2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># rand服从0-1均匀分布</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0.9925, 0.4613, 0.1272],
        [0.6944, 0.3979, 0.4879]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 服从标准正态分布的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0.7202, -1.3448,  1.1586],
        [-1.3567, -0.6265,  0.6884]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(<span class="number">2</span>,<span class="number">3</span>,size=(<span class="number">2</span>,<span class="number">2</span>)) <span class="comment"># 均值为2，标准差为3的张量  指定正态分布</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-1.8283,  0.6573],
        [ 0.7957,  4.2425]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">4</span>]) <span class="comment"># 在1-10之间随机抽取整数，组成两行四列的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 8, 1, 3],
        [4, 6, 9, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">5</span>)  <span class="comment"># 默认每隔1取</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2, 3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">1</span>,<span class="number">5</span>,<span class="number">0.5</span>) <span class="comment"># 从1-5 左闭右开 每隔0.5取值一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>) <span class="comment"># 从1-5 闭区间 等距取三个数</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 3., 5.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(<span class="number">1</span>,<span class="number">9</span>,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 2.6000, 4.2000, 5.8000, 7.4000, 9.0000])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[8.1717e+20, 2.1274e+23, 8.5032e+20],
        [1.0616e+21, 4.3445e-05, 2.0432e+20]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.full([<span class="number">2</span>,<span class="number">4</span>],<span class="number">2</span>)  <span class="comment"># 两行四列 填充2 的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 2, 2, 2],
        [2, 2, 2, 2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.full_like(t1,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint_like(t2,<span class="number">1</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([5, 1, 5])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.numpy()</span><br></pre></td></tr></table></figure>




<pre><code>array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int64)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(t1)</span><br></pre></td></tr></table></figure>




<pre><code>array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int64)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.tolist()</span><br></pre></td></tr></table></figure>




<pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(t1)   <span class="comment"># 一维张量由零维张量组成 二维由一维组成</span></span><br></pre></td></tr></table></figure>




<pre><code>[tensor(1),
 tensor(2),
 tensor(3),
 tensor(4),
 tensor(5),
 tensor(6),
 tensor(7),
 tensor(8),
 tensor(9),
 tensor(10)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t11=t1.clone()</span><br><span class="line">t11</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">0</span>]=<span class="number">200</span></span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t11)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([200,   2,   3,   4,   5,   6,   7,   8,   9,  10])
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<h2 id="张量的索引、分片、合并以及维度调整"><a href="#张量的索引、分片、合并以及维度调整" class="headerlink" title="张量的索引、分片、合并以及维度调整"></a>张量的索引、分片、合并以及维度调整</h2><pre><code>张量作为有序的序列，也是具备数值索引的功能，并且基本索引方法和python原生的列表、numpy中的数组基本一致，当然，所不同的是，pytorch中还定义了一种采用函数来进行索引的方式。
而作为pytorch中基本数据类型，张量即具备了列表、数组的基本功能，同时还充当着向量、矩阵、数据框等重要的数据结构，因此pytorch中也设置了非常完备的张量合并与变换的操作
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="一维张量索引"><a href="#一维张量索引" class="headerlink" title="一维张量索引"></a>一维张量索引</h4><p>一维张量的索引过程和python原生对象类型的索引一致,[start:end,step]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">1</span>:<span class="number">8</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 3, 4, 5, 6, 7, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">1</span>:<span class="number">8</span>:<span class="number">2</span>] <span class="comment"># 索引其中2-9号元素，左闭右开，每隔2个数取一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 4, 6, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">1</span>: :<span class="number">2</span>] <span class="comment"># 从第二个元素开始索引，一直到结尾，并且每隔两个数取一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2,  4,  6,  8, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[:<span class="number">8</span>:<span class="number">2</span>] <span class="comment"># 第一个元素到第九个元素（不包含）</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3, 5, 7])
</code></pre>
<p>在张量的索引中，step位必须大于0</p>
<h4 id="二维张量"><a href="#二维张量" class="headerlink" title="二维张量"></a>二维张量</h4><p>二维张量可以视为两个一维张量组合而成，在实际的索引过程中，需要用逗号进行分割，分别不表示对哪个一维张量进行索引，以及具体的一维张量的索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">1</span>,<span class="number">10</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2[<span class="number">0</span>,::<span class="number">2</span>]  <span class="comment"># 表示索引第一行、每隔两个元素取一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2[::<span class="number">2</span>,::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 3],
        [7, 9]])
</code></pre>
<h4 id="三维张量"><a href="#三维张量" class="headerlink" title="三维张量"></a>三维张量</h4><p> 即二维张量组成的三维张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">1</span>,<span class="number">28</span>).reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9]],

        [[10, 11, 12],
         [13, 14, 15],
         [16, 17, 18]],

        [[19, 20, 21],
         [22, 23, 24],
         [25, 26, 27]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([13, 14, 15])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[10, 11, 12],
        [13, 14, 15],
        [16, 17, 18]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>,::<span class="number">2</span>,::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[10, 12],
        [16, 18]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">indices=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">indices</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(t2,<span class="number">0</span>,indices) <span class="comment"># dim参数为0，代表在shape的第一个维度上索引 </span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(t2,<span class="number">1</span>,indices) <span class="comment"># dim参数为1，代表在shape的第二个维度上索引</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 3],
        [5, 6],
        [8, 9]])
</code></pre>
<h2 id="tensor-view-方法"><a href="#tensor-view-方法" class="headerlink" title="tensor.view()方法"></a>tensor.view()方法</h2><p>.view方法会返回一个类似视图的结果，通过。view方法还可以改变对象结构，生成一个不同结构，但是是“浅拷贝“</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te=t.view(<span class="number">3</span>,<span class="number">2</span>)    <span class="comment"># 构建一个结构不同但数据一样的“视图”</span></span><br><span class="line">te</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t[<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">t[<span class="number">1</span>][<span class="number">0</span>]=<span class="number">1</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 1, 1],
        [1, 4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">te   </span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 1],
        [1, 1],
        [4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr=t.view(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tr</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 1, 1],
         [1, 4, 5]]])
</code></pre>
<h2 id="张量的分片函数"><a href="#张量的分片函数" class="headerlink" title="张量的分片函数"></a>张量的分片函数</h2><h4 id="1-分块-chunk函数"><a href="#1-分块-chunk函数" class="headerlink" title="1. 分块 chunk函数"></a>1. 分块 chunk函数</h4><p>chunk函数能够按照某维度，对张量进行均匀切分，并返回原张量的视图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.chunk(t2,<span class="number">4</span>,dim=<span class="number">0</span>) <span class="comment"># 在第零个维度上进行四等分</span></span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2]]),
 tensor([[3, 4, 5]]),
 tensor([[6, 7, 8]]),
 tensor([[ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tc[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tc[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 1, 2]]),
 tensor([[3, 4, 5]]),
 tensor([[6, 7, 8]]),
 tensor([[ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.size()  <span class="comment"># dim=0 -&gt; 4   dim=1 -&gt; 3</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([4, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.chunk(t2,<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1],
         [3],
         [6],
         [9]]),
 tensor([[ 1],
         [ 4],
         [ 7],
         [10]]),
 tensor([[ 2],
         [ 5],
         [ 8],
         [11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.chunk(t2,<span class="number">3</span>,dim=<span class="number">0</span>)  <span class="comment"># 当原张量不能均分时，会返回次一级均分的结果</span></span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 1, 2],
         [3, 4, 5]]),
 tensor([[ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(t2,<span class="number">4</span>) <span class="comment"># 默认dim=0</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 1, 2]]),
 tensor([[3, 4, 5]]),
 tensor([[6, 7, 8]]),
 tensor([[ 9, 10, 11]]))
</code></pre>
<h4 id="2-拆分：split函数"><a href="#2-拆分：split函数" class="headerlink" title="2.拆分：split函数"></a>2.拆分：split函数</h4><p>split既能进行均分也能进行自定义切分，当然，和chunk函数一样，都是返回视图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.split(t2,<span class="number">2</span>,<span class="number">0</span>) <span class="comment"># 第二个参数表示切分的序列  第三个参数表示维度</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2],
         [3, 4, 5]]),
 tensor([[ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.split(t2,[<span class="number">1</span>,<span class="number">3</span>],<span class="number">0</span>)</span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2]]),
 tensor([[ 3,  4,  5],
         [ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(t2)  <span class="comment"># 序列的和与len()一致</span></span><br></pre></td></tr></table></figure>




<pre><code>4
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc[<span class="number">1</span>][<span class="number">0</span>]=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2]]),
 tensor([[ 1,  2,  3],
         [ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<h4 id="3-拼接"><a href="#3-拼接" class="headerlink" title="3.拼接"></a>3.拼接</h4><p>张量的合并操作类似于列表的追加元素，可以拼接、也可以堆叠</p>
<ul>
<li>拼接函数：cat</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b=torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c=torch.zeros(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([a,b])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([a,b],<span class="number">1</span>) <span class="comment"># 按照维度进行拼接，dim默认为0</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 1., 1., 1.],
        [0., 0., 0., 1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.car([a,c],<span class="number">1</span>) <span class="comment"># 形状不匹配时会报错</span></span><br></pre></td></tr></table></figure>

<ul>
<li>堆叠函数：stack<br>和拼接不同，堆叠不是将元素拆分重组，而是简单的将各参与堆叠的对象分装到一个更高维度的张量里<br>堆叠要求张量形状都相同</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.stack([torch.cat([a,c]),torch.cat([b,c])])</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 5, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack([a,b])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack([a,b]).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 2, 3])
</code></pre>
<h2 id="张量维度变换"><a href="#张量维度变换" class="headerlink" title="张量维度变换"></a>张量维度变换</h2><p>通过reshape方法可以灵活调整张量的形状，在实际使用中往往要使用升维和降维操作<br>使用squeeze函数进行降维，使用unsqueeze函数进行升维</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.arange(<span class="number">4</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a2=a.reshape(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">a2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2, 3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(a2).ndim</span><br></pre></td></tr></table></figure>




<pre><code>1
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a2=torch.zeros(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">a2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[0., 0.]],

         [[0., 0.]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">0</span>)  <span class="comment"># 在第1个维度索引上升高一个维度</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[[0., 0.]],

          [[0., 0.]]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">0</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 1, 2, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">2</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 2, 1, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">3</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 2, 1, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">4</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 2, 1, 2, 1])
</code></pre>
<h2 id="x3D-x3D-x3D-x3D-x3D-x3D-x3D"><a href="#x3D-x3D-x3D-x3D-x3D-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</h2><p>title: tensor基础<br>author: xmcchv<br>date: 2021-07-24 17:39:29<br>tags:</p>
<ul>
<li>pytorch</li>
</ul>
<hr>
<h2 id="张量的创建与常用方法-1"><a href="#张量的创建与常用方法-1" class="headerlink" title="张量的创建与常用方法"></a>张量的创建与常用方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a1=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">a1</span><br></pre></td></tr></table></figure>




<pre><code>array([[1, 2, 3],
       [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.tensor([a1,a1])</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 2, 3],
         [4, 5, 6]],

        [[1, 2, 3],
         [4, 5, 6]]], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.ndim</span><br></pre></td></tr></table></figure>




<pre><code>3
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(t3)</span><br></pre></td></tr></table></figure>




<pre><code>2
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.flatten()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 转化为两行，一列的张量</span></span><br><span class="line">t1.reshape(<span class="number">2</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1],
        [2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.reshape(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape([<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1],
        [2],
        [3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.reshape(<span class="number">12</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.reshape(<span class="number">2</span>,<span class="number">6</span>) <span class="comment"># 转换为两行六列的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3, 4, 5, 6],
        [1, 2, 3, 4, 5, 6]], dtype=torch.int32)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(<span class="number">5</span>)  <span class="comment"># 单位矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0.],
        [0., 0., 1., 0., 0.],
        [0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1</span><br><span class="line">torch.diag(t1) <span class="comment"># 不能使用list直接创建对角矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 0],
        [0, 2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># rand服从0-1均匀分布</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0.9925, 0.4613, 0.1272],
        [0.6944, 0.3979, 0.4879]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 服从标准正态分布的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0.7202, -1.3448,  1.1586],
        [-1.3567, -0.6265,  0.6884]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(<span class="number">2</span>,<span class="number">3</span>,size=(<span class="number">2</span>,<span class="number">2</span>)) <span class="comment"># 均值为2，标准差为3的张量  指定正态分布</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-1.8283,  0.6573],
        [ 0.7957,  4.2425]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint(<span class="number">1</span>,<span class="number">10</span>,[<span class="number">2</span>,<span class="number">4</span>]) <span class="comment"># 在1-10之间随机抽取整数，组成两行四列的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 8, 1, 3],
        [4, 6, 9, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">5</span>)  <span class="comment"># 默认每隔1取</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2, 3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">1</span>,<span class="number">5</span>,<span class="number">0.5</span>) <span class="comment"># 从1-5 左闭右开 每隔0.5取值一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>) <span class="comment"># 从1-5 闭区间 等距取三个数</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 3., 5.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(<span class="number">1</span>,<span class="number">9</span>,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 2.6000, 4.2000, 5.8000, 7.4000, 9.0000])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[8.1717e+20, 2.1274e+23, 8.5032e+20],
        [1.0616e+21, 4.3445e-05, 2.0432e+20]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.full([<span class="number">2</span>,<span class="number">4</span>],<span class="number">2</span>)  <span class="comment"># 两行四列 填充2 的张量</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 2, 2, 2],
        [2, 2, 2, 2]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.full_like(t1,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randint_like(t2,<span class="number">1</span>,<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([5, 1, 5])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.numpy()</span><br></pre></td></tr></table></figure>




<pre><code>array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int64)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(t1)</span><br></pre></td></tr></table></figure>




<pre><code>array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int64)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.tolist()</span><br></pre></td></tr></table></figure>




<pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span>(t1)   <span class="comment"># 一维张量由零维张量组成 二维由一维组成</span></span><br></pre></td></tr></table></figure>




<pre><code>[tensor(1),
 tensor(2),
 tensor(3),
 tensor(4),
 tensor(5),
 tensor(6),
 tensor(7),
 tensor(8),
 tensor(9),
 tensor(10)]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t11=t1.clone()</span><br><span class="line">t11</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">0</span>]=<span class="number">200</span></span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t11)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([200,   2,   3,   4,   5,   6,   7,   8,   9,  10])
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<h2 id="张量的索引、分片、合并以及维度调整-1"><a href="#张量的索引、分片、合并以及维度调整-1" class="headerlink" title="张量的索引、分片、合并以及维度调整"></a>张量的索引、分片、合并以及维度调整</h2><pre><code>张量作为有序的序列，也是具备数值索引的功能，并且基本索引方法和python原生的列表、numpy中的数组基本一致，当然，所不同的是，pytorch中还定义了一种采用函数来进行索引的方式。
而作为pytorch中基本数据类型，张量即具备了列表、数组的基本功能，同时还充当着向量、矩阵、数据框等重要的数据结构，因此pytorch中也设置了非常完备的张量合并与变换的操作
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="一维张量索引-1"><a href="#一维张量索引-1" class="headerlink" title="一维张量索引"></a>一维张量索引</h4><p>一维张量的索引过程和python原生对象类型的索引一致,[start:end,step]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">11</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">1</span>:<span class="number">8</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 3, 4, 5, 6, 7, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">1</span>:<span class="number">8</span>:<span class="number">2</span>] <span class="comment"># 索引其中2-9号元素，左闭右开，每隔2个数取一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([2, 4, 6, 8])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[<span class="number">1</span>: :<span class="number">2</span>] <span class="comment"># 从第二个元素开始索引，一直到结尾，并且每隔两个数取一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2,  4,  6,  8, 10])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1[:<span class="number">8</span>:<span class="number">2</span>] <span class="comment"># 第一个元素到第九个元素（不包含）</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3, 5, 7])
</code></pre>
<p>在张量的索引中，step位必须大于0</p>
<h4 id="二维张量-1"><a href="#二维张量-1" class="headerlink" title="二维张量"></a>二维张量</h4><p>二维张量可以视为两个一维张量组合而成，在实际的索引过程中，需要用逗号进行分割，分别不表示对哪个一维张量进行索引，以及具体的一维张量的索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">1</span>,<span class="number">10</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2[<span class="number">0</span>,::<span class="number">2</span>]  <span class="comment"># 表示索引第一行、每隔两个元素取一个</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2[::<span class="number">2</span>,::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 3],
        [7, 9]])
</code></pre>
<h4 id="三维张量-1"><a href="#三维张量-1" class="headerlink" title="三维张量"></a>三维张量</h4><p> 即二维张量组成的三维张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">1</span>,<span class="number">28</span>).reshape(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9]],

        [[10, 11, 12],
         [13, 14, 15],
         [16, 17, 18]],

        [[19, 20, 21],
         [22, 23, 24],
         [25, 26, 27]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([13, 14, 15])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[10, 11, 12],
        [13, 14, 15],
        [16, 17, 18]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3[<span class="number">1</span>,::<span class="number">2</span>,::<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[10, 12],
        [16, 18]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">indices=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">indices</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(t2,<span class="number">0</span>,indices) <span class="comment"># dim参数为0，代表在shape的第一个维度上索引 </span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(t2,<span class="number">1</span>,indices) <span class="comment"># dim参数为1，代表在shape的第二个维度上索引</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2, 3],
        [5, 6],
        [8, 9]])
</code></pre>
<h2 id="tensor-view-方法-1"><a href="#tensor-view-方法-1" class="headerlink" title="tensor.view()方法"></a>tensor.view()方法</h2><p>.view方法会返回一个类似视图的结果，通过。view方法还可以改变对象结构，生成一个不同结构，但是是“浅拷贝“</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">te=t.view(<span class="number">3</span>,<span class="number">2</span>)    <span class="comment"># 构建一个结构不同但数据一样的“视图”</span></span><br><span class="line">te</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t[<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">t[<span class="number">1</span>][<span class="number">0</span>]=<span class="number">1</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 1, 1],
        [1, 4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">te   </span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 1],
        [1, 1],
        [4, 5]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tr=t.view(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">tr</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1, 1, 1],
         [1, 4, 5]]])
</code></pre>
<h2 id="张量的分片函数-1"><a href="#张量的分片函数-1" class="headerlink" title="张量的分片函数"></a>张量的分片函数</h2><h4 id="1-分块-chunk函数-1"><a href="#1-分块-chunk函数-1" class="headerlink" title="1. 分块 chunk函数"></a>1. 分块 chunk函数</h4><p>chunk函数能够按照某维度，对张量进行均匀切分，并返回原张量的视图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.chunk(t2,<span class="number">4</span>,dim=<span class="number">0</span>) <span class="comment"># 在第零个维度上进行四等分</span></span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2]]),
 tensor([[3, 4, 5]]),
 tensor([[6, 7, 8]]),
 tensor([[ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tc[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tc[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 1, 2]]),
 tensor([[3, 4, 5]]),
 tensor([[6, 7, 8]]),
 tensor([[ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.size()  <span class="comment"># dim=0 -&gt; 4   dim=1 -&gt; 3</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([4, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.chunk(t2,<span class="number">3</span>,dim=<span class="number">1</span>)</span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1],
         [3],
         [6],
         [9]]),
 tensor([[ 1],
         [ 4],
         [ 7],
         [10]]),
 tensor([[ 2],
         [ 5],
         [ 8],
         [11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.chunk(t2,<span class="number">3</span>,dim=<span class="number">0</span>)  <span class="comment"># 当原张量不能均分时，会返回次一级均分的结果</span></span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 1, 2],
         [3, 4, 5]]),
 tensor([[ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.chunk(t2,<span class="number">4</span>) <span class="comment"># 默认dim=0</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[1, 1, 2]]),
 tensor([[3, 4, 5]]),
 tensor([[6, 7, 8]]),
 tensor([[ 9, 10, 11]]))
</code></pre>
<h4 id="2-拆分：split函数-1"><a href="#2-拆分：split函数-1" class="headerlink" title="2.拆分：split函数"></a>2.拆分：split函数</h4><p>split既能进行均分也能进行自定义切分，当然，和chunk函数一样，都是返回视图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).reshape(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.split(t2,<span class="number">2</span>,<span class="number">0</span>) <span class="comment"># 第二个参数表示切分的序列  第三个参数表示维度</span></span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2],
         [3, 4, 5]]),
 tensor([[ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc=torch.split(t2,[<span class="number">1</span>,<span class="number">3</span>],<span class="number">0</span>)</span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2]]),
 tensor([[ 3,  4,  5],
         [ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(t2)  <span class="comment"># 序列的和与len()一致</span></span><br></pre></td></tr></table></figure>




<pre><code>4
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tc[<span class="number">1</span>][<span class="number">0</span>]=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tc</span><br></pre></td></tr></table></figure>




<pre><code>(tensor([[0, 1, 2]]),
 tensor([[ 1,  2,  3],
         [ 6,  7,  8],
         [ 9, 10, 11]]))
</code></pre>
<h4 id="3-拼接-1"><a href="#3-拼接-1" class="headerlink" title="3.拼接"></a>3.拼接</h4><p>张量的合并操作类似于列表的追加元素，可以拼接、也可以堆叠</p>
<ul>
<li>拼接函数：cat</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b=torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c=torch.zeros(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([a,b])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat([a,b],<span class="number">1</span>) <span class="comment"># 按照维度进行拼接，dim默认为0</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 1., 1., 1.],
        [0., 0., 0., 1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.car([a,c],<span class="number">1</span>) <span class="comment"># 形状不匹配时会报错</span></span><br></pre></td></tr></table></figure>

<ul>
<li>堆叠函数：stack<br>和拼接不同，堆叠不是将元素拆分重组，而是简单的将各参与堆叠的对象分装到一个更高维度的张量里<br>堆叠要求张量形状都相同</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.stack([torch.cat([a,c]),torch.cat([b,c])])</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.],
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 5, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack([a,b])</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack([a,b]).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 2, 3])
</code></pre>
<h2 id="张量维度变换-1"><a href="#张量维度变换-1" class="headerlink" title="张量维度变换"></a>张量维度变换</h2><p>通过reshape方法可以灵活调整张量的形状，在实际使用中往往要使用升维和降维操作<br>使用squeeze函数进行降维，使用unsqueeze函数进行升维</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=torch.arange(<span class="number">4</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a2=a.reshape(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">a2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2, 3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.squeeze(a2).ndim</span><br></pre></td></tr></table></figure>




<pre><code>1
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a2=torch.zeros(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">a2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[0., 0.]],

         [[0., 0.]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">0</span>)  <span class="comment"># 在第1个维度索引上升高一个维度</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[[0., 0.]],

          [[0., 0.]]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">0</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 1, 2, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">2</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 2, 1, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">3</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 2, 1, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(a2,dim=<span class="number">4</span>).shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([1, 2, 1, 2, 1])
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>285b814601d5399a58128ce8798b99dda8ed6d59</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

                </div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                    </div>
                    <div>
                        
                    </div>
                </div>

                

                
                    <div class="article-nav">
                        
                            <div class="article-prev">
                                <a class="prev"
                                   rel="prev"
                                   href="/2024/10/27/%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/"
                                   title=""
                                >
                                    <span class="left arrow-icon flex-center">
                                      <i class="fas fa-chevron-left"></i>
                                    </span>
                                            <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis"></span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="article-next">
                                <a class="next"
                                   rel="next"
                                   href="/2024/10/27/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E8%AF%86%E5%88%AB%E8%8A%B1%E6%9C%B5/"
                                   title=""
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis"></span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                            <span class="right arrow-icon flex-center">
                                      <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="border-box website-info-box default">
        
            <div class="copyright-info info-item default">
                &copy;&nbsp;<span>2020</span>&nbsp;-&nbsp;2024
                
                    &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">Keep Team</a>
                
            </div>

            <div class="theme-info info-item default">
                由&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;驱动&nbsp;&&nbsp;主题&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
            </div>

            

            
        

        <div class="count-item info-item default">
            

            

            
        </div>
    </div>
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="tools-list border-box">
        <!-- PC TOC show toggle -->
        

        <!-- PC go comment -->
        
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    

    <!-- tablet toc -->
    
</main>



<!-- common -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local-search -->


<!-- code-block -->


<!-- lazyload -->


<div class="">
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        

        <!-- copyright-info -->
        

        <!-- share -->
        
    

    <!-- category-page -->
    

    <!-- links-page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->



</body>
</html>
