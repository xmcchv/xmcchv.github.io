<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Keep Team">
    
    <title>
        
        Keep Theme
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/logo.svg">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"xmcchv.github.io","root":"/","language":"zh-CN","path":"search.xml"}
    KEEP.theme_config = {"toc":{"enable":false,"number":false,"expand_all":false,"init_open":true,"layout":"right"},"style":{"primary_color":"#0066cc","logo":"/images/logo.svg","favicon":"/images/logo.svg","avatar":"/images/avatar.svg","first_screen":{"enable":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving.","hitokoto":false},"scroll":{"progress_bar":false,"percent":false}},"local_search":{"enable":false,"preload":false},"code_block":{"tools":{"enable":false,"style":"default"},"highlight_theme":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.21"},"waline":{"server_url":null,"reaction":false,"version":2},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"wordcount":false,"min2read":false},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":false,"share":false,"reward":{"enable":false,"img_link":null,"text":null}},"website_count":{"busuanzi_count":{"enable":false,"site_uv":false,"site_pv":false,"page_pv":false}},"version":"3.8.6"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container border-box">

    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.svg">
                </a>
            
            <a class="site-name border-box" href="/">
               Keep Theme
            </a>
        </div>

        <div class="right border-box">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">

                

                    <div class="fade-in-down-animation">
    <div class="post-page-container border-box">

        <div class="article-content-container border-box">

            

            <div class="article-content-bottom border-box">
                
                    <div class="article-title">
                        
                    </div>
                

                
                    <div class="article-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.svg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author">
                                <span class="name">Keep Team</span>
                                
                                    <span class="author-label">Lv3</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="article-meta-info-container border-box post">
    <div class="article-meta-info border-box">
        


        
            <span class="meta-info-item article-create-date">
                <i class="icon fa-solid fa-calendar-check"></i>&nbsp;
                <span class="pc">2024-10-27 00:42:49</span>
                <span class="mobile">2024-10-27 00:42</span>
            </span>

            <span class="meta-info-item article-update-date">
                <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                <span class="pc" data-updated="Sun Oct 27 2024 00:42:49 GMT+0800">2024-10-27 00:42:49</span>
            </span>
        

        

        

        
        
        
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="article-content keep-markdown-body">
                    

                    <h2 id="lt-lt-lt-lt-lt-lt-lt-HEAD"><a href="#lt-lt-lt-lt-lt-lt-lt-HEAD" class="headerlink" title="&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD"></a>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</h2><p>title: 张量的线性代数运算<br>date: 2021-07-31 23:14:20<br>mathjax: True<br>tags:</p>
<ul>
<li>pytorch</li>
</ul>
<hr>
<h2 id="张量的线性代数运算"><a href="#张量的线性代数运算" class="headerlink" title="张量的线性代数运算"></a>张量的线性代数运算</h2><ul>
<li>pytorch中BLAS和LAPACK模块的相关运算</li>
<li>pytorch中没有设置单独的矩阵对象类型，因此，在pytorch中二维张量就相当于矩阵对象，并且拥有一系列线性代数相关函数和方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h3 id="一、BLAS和LAPACK概览"><a href="#一、BLAS和LAPACK概览" class="headerlink" title="一、BLAS和LAPACK概览"></a>一、BLAS和LAPACK概览</h3><ul>
<li>矩阵的新编及特殊矩阵的构造方法：包括矩阵的转置、对角矩阵的创建、单位矩阵的创建、上&#x2F;下三角矩阵的创建等;</li>
<li>矩阵的基本运算:矩阵乘法、向量內积、矩阵和向量的乘法等</li>
<li>矩阵的线性代数运算:包括矩阵的迹、矩阵的秩、逆矩阵的求解、伴随矩阵和广义逆矩阵等</li>
<li>矩阵分解运算:特征分解、奇异值分解和SVD分解等</li>
</ul>
<h3 id="二、矩阵的形态及特殊矩阵构造方法"><a href="#二、矩阵的形态及特殊矩阵构造方法" class="headerlink" title="二、矩阵的形态及特殊矩阵构造方法"></a>二、矩阵的形态及特殊矩阵构造方法</h3><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.t(t)</td>
<td align="left">t转置</td>
</tr>
<tr>
<td align="left">torch.eye(n)</td>
<td align="left">创建包含n个分量的单位矩阵</td>
</tr>
<tr>
<td align="left">torch.diag(t1)</td>
<td align="left">以t1中各元素创建对角矩阵</td>
</tr>
<tr>
<td align="left">torch.triu(t)</td>
<td align="left">去矩阵t中上三角矩阵</td>
</tr>
<tr>
<td align="left">torch.tril(t)</td>
<td align="left">去矩阵t中下三角矩阵</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>).<span class="built_in">float</span>()</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.t(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.t()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">5</span>)</span><br><span class="line">torch.diag(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0],
        [0, 0, 2, 0, 0],
        [0, 0, 0, 3, 0],
        [0, 0, 0, 0, 4]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">torch.triu(t1) <span class="comment"># 上三角矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [0, 4, 5],
        [0, 0, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tril(t1) <span class="comment"># 下三角矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 0, 0],
        [3, 4, 0],
        [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.triu(t1,-<span class="number">1</span>) <span class="comment"># 上三角矩阵向左下偏移一位</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5],
        [0, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.triu(t1,<span class="number">1</span>) <span class="comment"># 上三角矩阵向上偏移一位</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [0, 0, 5],
        [0, 0, 0]])
</code></pre>
<h3 id="三、矩阵的基本运算"><a href="#三、矩阵的基本运算" class="headerlink" title="三、矩阵的基本运算"></a>三、矩阵的基本运算</h3><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.dot(t1,t2)</td>
<td align="left">计算t1、t2张量內积</td>
</tr>
<tr>
<td align="left">torch.mm(t1,t2)</td>
<td align="left">矩阵乘法</td>
</tr>
<tr>
<td align="left">torch.mv(t1,t2)</td>
<td align="left">矩阵乘向量</td>
</tr>
<tr>
<td align="left">torch.bmm(t1,t2)</td>
<td align="left">批量矩阵乘法</td>
</tr>
<tr>
<td align="left">torch.addmm(t,t1,t2)</td>
<td align="left">矩阵相乘后相加</td>
</tr>
<tr>
<td align="left">torch.addbmm(t,t1,t2)</td>
<td align="left">批量矩阵相乘后相加</td>
</tr>
</tbody></table>
<ul>
<li>dot\vdot:点积计算<br>在pytorch中，dot和vdot只能作用于一维张量，对于数值型对象，二者计算结果没有区别，在复数运算时有区别</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(t,t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.vdot(t,t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14)
</code></pre>
<ul>
<li>mm:矩阵乘法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">1</span>,<span class="number">10</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*t1 <span class="comment"># 对应元素位置相乘</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 1,  4,  9],
        [16, 25, 36]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(t1,t2) <span class="comment"># 矩阵乘法 ab bc -&gt; ac</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[30, 36, 42],
        [66, 81, 96]])
</code></pre>
<ul>
<li>mv:矩阵和向量相乘<br>先将向量转为列向量然后再相乘</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">met=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">met</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vec=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">vec</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mv(met,vec) <span class="comment"># 矩阵的列数和向量的元素个数相同</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([14, 32])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vec.reshape(<span class="number">3</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1],
        [2],
        [3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(met,vec.reshape(<span class="number">3</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[14],
        [32]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(met,vec.reshape(<span class="number">3</span>,<span class="number">1</span>)).flatten()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([14, 32])
</code></pre>
<p>mv函数本质上提供了一种二维张量和一维张量相乘的方法，再线性代数运算过程中，有很多矩阵乘向量的场景，典型的如线性回归的求解过程，通常情况下我们需要将向量转化位列向量然后进行计算，但pytorch中单独设置了一个矩阵和向量相乘的方法，从而简化了行&#x2F;列向量的理解过程和将向量转为列向量的转化过程</p>
<ul>
<li>bmm:批量矩阵相乘<br>指三维张量的矩阵乘法，三维张量内各对应位置的矩阵相乘</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">1</span>,<span class="number">13</span>).reshape(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2],
         [ 3,  4]],

        [[ 5,  6],
         [ 7,  8]],

        [[ 9, 10],
         [11, 12]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t4=torch.arange(<span class="number">1</span>,<span class="number">19</span>).reshape(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t4</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2,  3],
         [ 4,  5,  6]],

        [[ 7,  8,  9],
         [10, 11, 12]],

        [[13, 14, 15],
         [16, 17, 18]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bmm(t3,t4)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[  9,  12,  15],
         [ 19,  26,  33]],

        [[ 95, 106, 117],
         [129, 144, 159]],

        [[277, 296, 315],
         [335, 358, 381]]])
</code></pre>
<p>point:</p>
<ol>
<li>三维张量包含的矩阵个数需要相同</li>
<li>每个内部矩阵，需要满足矩阵乘法的条件，ab bc -&gt; ac</li>
</ol>
<ul>
<li>addmm:矩阵相乘后相加<br>addmm函数结构：addmm(input,mat1,mat2,beta&#x3D;1,alpha&#x3D;1)<br>输出结果：beta*input+alpha*(mat1*mat2)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">3</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[30, 36, 42],
        [66, 81, 96]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addmm(t,t1,t2) <span class="comment"># t1*t2 + t</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[30, 37, 44],
        [66, 82, 98]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addmm(t,t1,t2,beta=<span class="number">0</span>,alpha=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[300, 360, 420],
        [660, 810, 960]])
</code></pre>
<h3 id="四、矩阵的线性代数运算"><a href="#四、矩阵的线性代数运算" class="headerlink" title="四、矩阵的线性代数运算"></a>四、矩阵的线性代数运算</h3><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.trace(A)</td>
<td align="left">矩阵的迹</td>
</tr>
<tr>
<td align="left">matrix_rank(A)</td>
<td align="left">矩阵的秩</td>
</tr>
<tr>
<td align="left">torch.det(A)</td>
<td align="left">计算矩阵A的行列式</td>
</tr>
<tr>
<td align="left">torch.inverse(A)</td>
<td align="left">矩阵求逆</td>
</tr>
<tr>
<td align="left">torch.lstsq(A,B)</td>
<td align="left">最小二乘法</td>
</tr>
</tbody></table>
<ol>
<li>矩阵的迹（trace）<br>矩阵的迹就是计算矩阵对角线元素之和</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">2</span>,<span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [4., 5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.trace(A)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(6.)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">B</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.trace(B) <span class="comment"># 对于矩阵的迹来说，计算过程不需要是方阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor(6)
</code></pre>
<ol start="2">
<li>矩阵的秩（rank）<br>矩阵的秩是指矩阵中行或列的极大线性无关组个数，矩阵的秩唯一</li>
</ol>
<ul>
<li>matrix_rank计算矩阵的秩</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">1</span>,<span class="number">5</span>).reshape(<span class="number">2</span>,<span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matrix_rank(A)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>]]).<span class="built_in">float</span>()</span><br><span class="line">B</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [2., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matrix_rank(B)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(1)
</code></pre>
<ol start="3">
<li>矩阵的行列式（det)</li>
</ol>
<p>行列式作为一个基本数学工具，实际上就是进行线性变换的伸缩因子<br>对于任何一个n维方阵，行列式的计算过程如下：</p>
<p>$$D&#x3D;\begin{vmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}\end{vmatrix}$$</p>
<p>$$D&#x3D;\sum (-1)^ka_{1k_1}a_{2k_2}\cdots a_{nk_n}$$</p>
<p>对于一个2*2的矩阵，行列式的计算就是主对角线元素之积减去另外两个元素之积</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">4</span>,<span class="number">5</span>]]).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [4., 5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.det(A)   <span class="comment"># ad-bc</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor(-3.)
</code></pre>
<ol start="4">
<li>线性方程组的矩阵表达形式</li>
</ol>
<p>矩阵-》由向量组组成的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">1</span>,<span class="number">5</span>).reshape(<span class="number">2</span>,<span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>],<span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x22856f5ae80&gt;]
</code></pre>
<p><img src="/images/output_55_1.png" alt="png"></p>
<p>如果更进一步，我们希望在二维空间中找到一条直线，来拟合这两个点，也就是构建一个线性回归模型，我们可以设置线性回归方程如下：</p>
<p>$$y&#x3D;ax+b$$</p>
<p>带入(1,2)和(3,4)两个点后，我们还可以进一步将表达式改写成矩阵表示形式。<br>A*x&#x3D;B</p>
<p><img src="/images/Snipaste_2021-08-01_20-03-43.png" alt="&quot;矩阵表示形式&quot;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.tensor([[<span class="number">1.0</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">1</span>]])</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1.],
        [3., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B=torch.tensor([<span class="number">2.0</span>,<span class="number">4</span>])</span><br><span class="line">B</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2., 4.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.inverse(A) <span class="comment"># 求逆矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.5000,  0.5000],
        [ 1.5000, -0.5000]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(torch.inverse(A),A)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 1.0000e+00, -5.9605e-08],
        [-1.1921e-07,  1.0000e+00]])
</code></pre>
<p>$$x&#x3D;A^{-1}*B$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mv(torch.inverse(A),B)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.0000])
</code></pre>
<p>a&#x3D;1,b&#x3D;1<br>最终得到线性方程为$y&#x3D;x+1$</p>
<h3 id="五、矩阵的分解"><a href="#五、矩阵的分解" class="headerlink" title="五、矩阵的分解"></a>五、矩阵的分解</h3><p>矩阵的分解也是矩阵运算中常规计算，矩阵分解有很多种类，常见的例如QR分解、LU分解、特征分解和SVD分解，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积。分解之后的等式如下：</p>
<p>$$A&#x3D;V \cup D$$</p>
<ol>
<li>特征分解<br>特征分解中，矩阵分解形式为：</li>
</ol>
<p>$$A &#x3D; Q \Lambda Q^{-1}$$</p>
<p>其中Q和$Q^{-1}$互为逆矩阵，Q的列就是A的特征值对应的特征向量，$\Lambda$就是特征值组成的对角矩阵</p>
<ul>
<li>torch.eig函数: 特征分解</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">1</span>,<span class="number">10</span>).reshape(<span class="number">3</span>,<span class="number">3</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eig(A,eigenvectors=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.eig(
eigenvalues=tensor([[ 1.6117e+01,  0.0000e+00],
        [-1.1168e+00,  0.0000e+00],
        [ 2.9486e-07,  0.0000e+00]]),
eigenvectors=tensor([[-0.2320, -0.7858,  0.4082],
        [-0.5253, -0.0868, -0.8165],
        [-0.8187,  0.6123,  0.4082]]))
</code></pre>
<ol start="2">
<li>奇异值分解(SVD)<br>奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为mxn的奇异矩阵A分解为三个部分：</li>
</ol>
<p>$$A &#x3D; U \sum V^{T}$$</p>
<p>其中U、V是两个正交矩阵，其中的每一行(列)分别被称为左奇异向量和右奇异向量，他们和$\sum$中对角线上的奇异值相对应，通常情况下我们只保留前k和奇异向量和奇异值。其中U是mxk矩阵，V是nxk矩阵，$\sum$是kxk的方阵，从而减少存储空间的效果</p>
<p>$$A_{m<em>n} &#x3D; U_{m</em>n} \sum <em>{m*n} V</em>{n<em>n}^T \approx U_{m</em>k} \sum <em>{k*k}V</em>{k*n}^T$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C=torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],[<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>],[<span class="number">3.</span>, <span class="number">6.</span>, <span class="number">9.</span>]])</span><br><span class="line">C</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2., 3.],
        [2., 4., 6.],
        [3., 6., 9.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.svd(C)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.svd(
U=tensor([[-2.6726e-01,  9.6362e-01, -3.7767e-08],
        [-5.3452e-01, -1.4825e-01, -8.3205e-01],
        [-8.0178e-01, -2.2237e-01,  5.5470e-01]]),
S=tensor([1.4000e+01, 4.2751e-08, 1.6397e-15]),
V=tensor([[-0.2673, -0.9636,  0.0000],
        [-0.5345,  0.1482, -0.8321],
        [-0.8018,  0.2224,  0.5547]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CU,CS,CV=torch.svd(C)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.diag(CS)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.4000e+01, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 4.2751e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6397e-15]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(torch.mm(CU,torch.diag(CS)),CV.t())</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.0000, 2.0000, 3.0000],
        [2.0000, 4.0000, 6.0000],
        [3.0000, 6.0000, 9.0000]])
</code></pre>
<p>能够看出，上述输出完成还原了C矩阵，此时我们可根据svd输出结果对C进行降维，此时C可只保留第一列，即k&#x3D;1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">U1=CU[:,<span class="number">0</span>].reshape(<span class="number">3</span>,<span class="number">1</span>) <span class="comment"># U的第一列</span></span><br><span class="line">U1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.2673],
        [-0.5345],
        [-0.8018]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C1=CS[<span class="number">0</span>] <span class="comment"># C的第一个值</span></span><br><span class="line">C1</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14.0000)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">V1=CV[:,<span class="number">0</span>].reshape(<span class="number">1</span>,<span class="number">3</span>) <span class="comment"># V 的第一行</span></span><br><span class="line">V1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.2673, -0.5345, -0.8018]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm((U1*C1),V1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.0000, 2.0000, 3.0000],
        [2.0000, 4.0000, 6.0000],
        [3.0000, 6.0000, 9.0000]])
</code></pre>
<h1 id="此时输出的Cd矩阵以及和原矩阵高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫"><a href="#此时输出的Cd矩阵以及和原矩阵高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫" class="headerlink" title="此时输出的Cd矩阵以及和原矩阵高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫"></a>此时输出的Cd矩阵以及和原矩阵高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫</h1><hr>
<p>title: 张量的线性代数运算<br>date: 2021-07-31 23:14:20<br>mathjax: True<br>tags:</p>
<ul>
<li>pytorch</li>
</ul>
<hr>
<h2 id="张量的线性代数运算-1"><a href="#张量的线性代数运算-1" class="headerlink" title="张量的线性代数运算"></a>张量的线性代数运算</h2><ul>
<li>pytorch中BLAS和LAPACK模块的相关运算</li>
<li>pytorch中没有设置单独的矩阵对象类型，因此，在pytorch中二维张量就相当于矩阵对象，并且拥有一系列线性代数相关函数和方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h3 id="一、BLAS和LAPACK概览-1"><a href="#一、BLAS和LAPACK概览-1" class="headerlink" title="一、BLAS和LAPACK概览"></a>一、BLAS和LAPACK概览</h3><ul>
<li>矩阵的新编及特殊矩阵的构造方法：包括矩阵的转置、对角矩阵的创建、单位矩阵的创建、上&#x2F;下三角矩阵的创建等;</li>
<li>矩阵的基本运算:矩阵乘法、向量內积、矩阵和向量的乘法等</li>
<li>矩阵的线性代数运算:包括矩阵的迹、矩阵的秩、逆矩阵的求解、伴随矩阵和广义逆矩阵等</li>
<li>矩阵分解运算:特征分解、奇异值分解和SVD分解等</li>
</ul>
<h3 id="二、矩阵的形态及特殊矩阵构造方法-1"><a href="#二、矩阵的形态及特殊矩阵构造方法-1" class="headerlink" title="二、矩阵的形态及特殊矩阵构造方法"></a>二、矩阵的形态及特殊矩阵构造方法</h3><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.t(t)</td>
<td align="left">t转置</td>
</tr>
<tr>
<td align="left">torch.eye(n)</td>
<td align="left">创建包含n个分量的单位矩阵</td>
</tr>
<tr>
<td align="left">torch.diag(t1)</td>
<td align="left">以t1中各元素创建对角矩阵</td>
</tr>
<tr>
<td align="left">torch.triu(t)</td>
<td align="left">去矩阵t中上三角矩阵</td>
</tr>
<tr>
<td align="left">torch.tril(t)</td>
<td align="left">去矩阵t中下三角矩阵</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>).<span class="built_in">float</span>()</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.t(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1.t()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">5</span>)</span><br><span class="line">torch.diag(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 0, 0, 0, 0],
        [0, 1, 0, 0, 0],
        [0, 0, 2, 0, 0],
        [0, 0, 0, 3, 0],
        [0, 0, 0, 0, 4]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">torch.triu(t1) <span class="comment"># 上三角矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [0, 4, 5],
        [0, 0, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tril(t1) <span class="comment"># 下三角矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 0, 0],
        [3, 4, 0],
        [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.triu(t1,-<span class="number">1</span>) <span class="comment"># 上三角矩阵向左下偏移一位</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5],
        [0, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.triu(t1,<span class="number">1</span>) <span class="comment"># 上三角矩阵向上偏移一位</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [0, 0, 5],
        [0, 0, 0]])
</code></pre>
<h3 id="三、矩阵的基本运算-1"><a href="#三、矩阵的基本运算-1" class="headerlink" title="三、矩阵的基本运算"></a>三、矩阵的基本运算</h3><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.dot(t1,t2)</td>
<td align="left">计算t1、t2张量內积</td>
</tr>
<tr>
<td align="left">torch.mm(t1,t2)</td>
<td align="left">矩阵乘法</td>
</tr>
<tr>
<td align="left">torch.mv(t1,t2)</td>
<td align="left">矩阵乘向量</td>
</tr>
<tr>
<td align="left">torch.bmm(t1,t2)</td>
<td align="left">批量矩阵乘法</td>
</tr>
<tr>
<td align="left">torch.addmm(t,t1,t2)</td>
<td align="left">矩阵相乘后相加</td>
</tr>
<tr>
<td align="left">torch.addbmm(t,t1,t2)</td>
<td align="left">批量矩阵相乘后相加</td>
</tr>
</tbody></table>
<ul>
<li>dot\vdot:点积计算<br>在pytorch中，dot和vdot只能作用于一维张量，对于数值型对象，二者计算结果没有区别，在复数运算时有区别</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(t,t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.vdot(t,t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14)
</code></pre>
<ul>
<li>mm:矩阵乘法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">1</span>,<span class="number">10</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*t1 <span class="comment"># 对应元素位置相乘</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 1,  4,  9],
        [16, 25, 36]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(t1,t2) <span class="comment"># 矩阵乘法 ab bc -&gt; ac</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[30, 36, 42],
        [66, 81, 96]])
</code></pre>
<ul>
<li>mv:矩阵和向量相乘<br>先将向量转为列向量然后再相乘</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">met=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">met</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vec=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">vec</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mv(met,vec) <span class="comment"># 矩阵的列数和向量的元素个数相同</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([14, 32])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vec.reshape(<span class="number">3</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1],
        [2],
        [3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(met,vec.reshape(<span class="number">3</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[14],
        [32]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(met,vec.reshape(<span class="number">3</span>,<span class="number">1</span>)).flatten()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([14, 32])
</code></pre>
<p>mv函数本质上提供了一种二维张量和一维张量相乘的方法，再线性代数运算过程中，有很多矩阵乘向量的场景，典型的如线性回归的求解过程，通常情况下我们需要将向量转化位列向量然后进行计算，但pytorch中单独设置了一个矩阵和向量相乘的方法，从而简化了行&#x2F;列向量的理解过程和将向量转为列向量的转化过程</p>
<ul>
<li>bmm:批量矩阵相乘<br>指三维张量的矩阵乘法，三维张量内各对应位置的矩阵相乘</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">1</span>,<span class="number">13</span>).reshape(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2],
         [ 3,  4]],

        [[ 5,  6],
         [ 7,  8]],

        [[ 9, 10],
         [11, 12]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t4=torch.arange(<span class="number">1</span>,<span class="number">19</span>).reshape(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t4</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 1,  2,  3],
         [ 4,  5,  6]],

        [[ 7,  8,  9],
         [10, 11, 12]],

        [[13, 14, 15],
         [16, 17, 18]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.bmm(t3,t4)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[  9,  12,  15],
         [ 19,  26,  33]],

        [[ 95, 106, 117],
         [129, 144, 159]],

        [[277, 296, 315],
         [335, 358, 381]]])
</code></pre>
<p>point:</p>
<ol>
<li>三维张量包含的矩阵个数需要相同</li>
<li>每个内部矩阵，需要满足矩阵乘法的条件，ab bc -&gt; ac</li>
</ol>
<ul>
<li>addmm:矩阵相乘后相加<br>addmm函数结构：addmm(input,mat1,mat2,beta&#x3D;1,alpha&#x3D;1)<br>输出结果：beta*input+alpha*(mat1*mat2)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">3</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[30, 36, 42],
        [66, 81, 96]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addmm(t,t1,t2) <span class="comment"># t1*t2 + t</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[30, 37, 44],
        [66, 82, 98]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.addmm(t,t1,t2,beta=<span class="number">0</span>,alpha=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[300, 360, 420],
        [660, 810, 960]])
</code></pre>
<h3 id="四、矩阵的线性代数运算-1"><a href="#四、矩阵的线性代数运算-1" class="headerlink" title="四、矩阵的线性代数运算"></a>四、矩阵的线性代数运算</h3><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.trace(A)</td>
<td align="left">矩阵的迹</td>
</tr>
<tr>
<td align="left">matrix_rank(A)</td>
<td align="left">矩阵的秩</td>
</tr>
<tr>
<td align="left">torch.det(A)</td>
<td align="left">计算矩阵A的行列式</td>
</tr>
<tr>
<td align="left">torch.inverse(A)</td>
<td align="left">矩阵求逆</td>
</tr>
<tr>
<td align="left">torch.lstsq(A,B)</td>
<td align="left">最小二乘法</td>
</tr>
</tbody></table>
<ol>
<li>矩阵的迹（trace）<br>矩阵的迹就是计算矩阵对角线元素之和</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>]).reshape(<span class="number">2</span>,<span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [4., 5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.trace(A)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(6.)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B=torch.arange(<span class="number">1</span>,<span class="number">7</span>).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">B</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.trace(B) <span class="comment"># 对于矩阵的迹来说，计算过程不需要是方阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor(6)
</code></pre>
<ol start="2">
<li>矩阵的秩（rank）<br>矩阵的秩是指矩阵中行或列的极大线性无关组个数，矩阵的秩唯一</li>
</ol>
<ul>
<li>matrix_rank计算矩阵的秩</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">1</span>,<span class="number">5</span>).reshape(<span class="number">2</span>,<span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matrix_rank(A)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>]]).<span class="built_in">float</span>()</span><br><span class="line">B</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [2., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matrix_rank(B)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(1)
</code></pre>
<ol start="3">
<li>矩阵的行列式（det)</li>
</ol>
<p>行列式作为一个基本数学工具，实际上就是进行线性变换的伸缩因子<br>对于任何一个n维方阵，行列式的计算过程如下：</p>
<p>$$D&#x3D;\begin{vmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\ \cdots &amp; \cdots &amp; \cdots &amp; \cdots\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}\end{vmatrix}$$</p>
<p>$$D&#x3D;\sum (-1)^ka_{1k_1}a_{2k_2}\cdots a_{nk_n}$$</p>
<p>对于一个2*2的矩阵，行列式的计算就是主对角线元素之积减去另外两个元素之积</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">4</span>,<span class="number">5</span>]]).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [4., 5.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.det(A)   <span class="comment"># ad-bc</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor(-3.)
</code></pre>
<ol start="4">
<li>线性方程组的矩阵表达形式</li>
</ol>
<p>矩阵-》由向量组组成的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">1</span>,<span class="number">5</span>).reshape(<span class="number">2</span>,<span class="number">2</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2.],
        [3., 4.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(A[:,<span class="number">0</span>],A[:,<span class="number">1</span>],<span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure>




<pre><code>[&lt;matplotlib.lines.Line2D at 0x22856f5ae80&gt;]
</code></pre>
<p><img src="/images/output_55_1.png" alt="png"></p>
<p>如果更进一步，我们希望在二维空间中找到一条直线，来拟合这两个点，也就是构建一个线性回归模型，我们可以设置线性回归方程如下：</p>
<p>$$y&#x3D;ax+b$$</p>
<p>带入(1,2)和(3,4)两个点后，我们还可以进一步将表达式改写成矩阵表示形式。<br>A*x&#x3D;B</p>
<p><img src="/images/Snipaste_2021-08-01_20-03-43.png" alt="&quot;矩阵表示形式&quot;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.tensor([[<span class="number">1.0</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">1</span>]])</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1.],
        [3., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B=torch.tensor([<span class="number">2.0</span>,<span class="number">4</span>])</span><br><span class="line">B</span><br></pre></td></tr></table></figure>




<pre><code>tensor([2., 4.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.inverse(A) <span class="comment"># 求逆矩阵</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.5000,  0.5000],
        [ 1.5000, -0.5000]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(torch.inverse(A),A)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 1.0000e+00, -5.9605e-08],
        [-1.1921e-07,  1.0000e+00]])
</code></pre>
<p>$$x&#x3D;A^{-1}*B$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mv(torch.inverse(A),B)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.0000])
</code></pre>
<p>a&#x3D;1,b&#x3D;1<br>最终得到线性方程为$y&#x3D;x+1$</p>
<h3 id="五、矩阵的分解-1"><a href="#五、矩阵的分解-1" class="headerlink" title="五、矩阵的分解"></a>五、矩阵的分解</h3><p>矩阵的分解也是矩阵运算中常规计算，矩阵分解有很多种类，常见的例如QR分解、LU分解、特征分解和SVD分解，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积。分解之后的等式如下：</p>
<p>$$A&#x3D;V \cup D$$</p>
<ol>
<li>特征分解<br>特征分解中，矩阵分解形式为：</li>
</ol>
<p>$$A &#x3D; Q \Lambda Q^{-1}$$</p>
<p>其中Q和$Q^{-1}$互为逆矩阵，Q的列就是A的特征值对应的特征向量，$\Lambda$就是特征值组成的对角矩阵</p>
<ul>
<li>torch.eig函数: 特征分解</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A=torch.arange(<span class="number">1</span>,<span class="number">10</span>).reshape(<span class="number">3</span>,<span class="number">3</span>).<span class="built_in">float</span>()</span><br><span class="line">A</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eig(A,eigenvectors=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.eig(
eigenvalues=tensor([[ 1.6117e+01,  0.0000e+00],
        [-1.1168e+00,  0.0000e+00],
        [ 2.9486e-07,  0.0000e+00]]),
eigenvectors=tensor([[-0.2320, -0.7858,  0.4082],
        [-0.5253, -0.0868, -0.8165],
        [-0.8187,  0.6123,  0.4082]]))
</code></pre>
<ol start="2">
<li>奇异值分解(SVD)<br>奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为mxn的奇异矩阵A分解为三个部分：</li>
</ol>
<p>$$A &#x3D; U \sum V^{T}$$</p>
<p>其中U、V是两个正交矩阵，其中的每一行(列)分别被称为左奇异向量和右奇异向量，他们和$\sum$中对角线上的奇异值相对应，通常情况下我们只保留前k和奇异向量和奇异值。其中U是mxk矩阵，V是nxk矩阵，$\sum$是kxk的方阵，从而减少存储空间的效果</p>
<p>$$A_{m<em>n} &#x3D; U_{m</em>n} \sum <em>{m*n} V</em>{n<em>n}^T \approx U_{m</em>k} \sum <em>{k*k}V</em>{k*n}^T$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C=torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],[<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>],[<span class="number">3.</span>, <span class="number">6.</span>, <span class="number">9.</span>]])</span><br><span class="line">C</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 2., 3.],
        [2., 4., 6.],
        [3., 6., 9.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.svd(C)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.svd(
U=tensor([[-2.6726e-01,  9.6362e-01, -3.7767e-08],
        [-5.3452e-01, -1.4825e-01, -8.3205e-01],
        [-8.0178e-01, -2.2237e-01,  5.5470e-01]]),
S=tensor([1.4000e+01, 4.2751e-08, 1.6397e-15]),
V=tensor([[-0.2673, -0.9636,  0.0000],
        [-0.5345,  0.1482, -0.8321],
        [-0.8018,  0.2224,  0.5547]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CU,CS,CV=torch.svd(C)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.diag(CS)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.4000e+01, 0.0000e+00, 0.0000e+00],
        [0.0000e+00, 4.2751e-08, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6397e-15]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(torch.mm(CU,torch.diag(CS)),CV.t())</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.0000, 2.0000, 3.0000],
        [2.0000, 4.0000, 6.0000],
        [3.0000, 6.0000, 9.0000]])
</code></pre>
<p>能够看出，上述输出完成还原了C矩阵，此时我们可根据svd输出结果对C进行降维，此时C可只保留第一列，即k&#x3D;1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">U1=CU[:,<span class="number">0</span>].reshape(<span class="number">3</span>,<span class="number">1</span>) <span class="comment"># U的第一列</span></span><br><span class="line">U1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.2673],
        [-0.5345],
        [-0.8018]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C1=CS[<span class="number">0</span>] <span class="comment"># C的第一个值</span></span><br><span class="line">C1</span><br></pre></td></tr></table></figure>




<pre><code>tensor(14.0000)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">V1=CV[:,<span class="number">0</span>].reshape(<span class="number">1</span>,<span class="number">3</span>) <span class="comment"># V 的第一行</span></span><br><span class="line">V1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.2673, -0.5345, -0.8018]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm((U1*C1),V1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.0000, 2.0000, 3.0000],
        [2.0000, 4.0000, 6.0000],
        [3.0000, 6.0000, 9.0000]])
</code></pre>
<p>此时输出的Cd矩阵以及和原矩阵高度相似了，损失信息在R的计算中基本可以忽略不计，经过SVD分解，矩阵的信息能够被压缩至更小的空间内进行存储，从而为PCA（主成分分析）、LSI（潜在语义索引）等算法做好了数学工具层面的铺垫</p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>285b814601d5399a58128ce8798b99dda8ed6d59</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

                </div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                    </div>
                    <div>
                        
                    </div>
                </div>

                

                
                    <div class="article-nav">
                        
                            <div class="article-prev">
                                <a class="prev"
                                   rel="prev"
                                   href="/2024/10/27/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD%E5%92%8C%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97/"
                                   title=""
                                >
                                    <span class="left arrow-icon flex-center">
                                      <i class="fas fa-chevron-left"></i>
                                    </span>
                                            <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis"></span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="article-next">
                                <a class="next"
                                   rel="next"
                                   href="/2024/10/27/imgidentity/"
                                   title=""
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis"></span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                            <span class="right arrow-icon flex-center">
                                      <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="border-box website-info-box default">
        
            <div class="copyright-info info-item default">
                &copy;&nbsp;<span>2020</span>&nbsp;-&nbsp;2024
                
                    &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">Keep Team</a>
                
            </div>

            <div class="theme-info info-item default">
                由&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;驱动&nbsp;&&nbsp;主题&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
            </div>

            

            
        

        <div class="count-item info-item default">
            

            

            
        </div>
    </div>
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="tools-list border-box">
        <!-- PC TOC show toggle -->
        

        <!-- PC go comment -->
        
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    

    <!-- tablet toc -->
    
</main>



<!-- common -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local-search -->


<!-- code-block -->


<!-- lazyload -->


<div class="">
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        

        <!-- copyright-info -->
        

        <!-- share -->
        
    

    <!-- category-page -->
    

    <!-- links-page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->



</body>
</html>
