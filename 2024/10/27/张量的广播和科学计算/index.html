<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="Keep Team">
    
    <title>
        
        Keep Theme
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/logo.svg">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"xmcchv.github.io","root":"/","language":"zh-CN","path":"search.xml"}
    KEEP.theme_config = {"toc":{"enable":false,"number":false,"expand_all":false,"init_open":true,"layout":"right"},"style":{"primary_color":"#0066cc","logo":"/images/logo.svg","favicon":"/images/logo.svg","avatar":"/images/avatar.svg","first_screen":{"enable":false,"background_img":"/images/bg.svg","description":"Keep writing and Keep loving.","hitokoto":false},"scroll":{"progress_bar":false,"percent":false}},"local_search":{"enable":false,"preload":false},"code_block":{"tools":{"enable":false,"style":"default"},"highlight_theme":"default"},"pjax":{"enable":false},"lazyload":{"enable":false},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.21"},"waline":{"server_url":null,"reaction":false,"version":2},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false}},"post":{"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect"]},"word_count":{"wordcount":false,"min2read":false},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":false,"share":false,"reward":{"enable":false,"img_link":null,"text":null}},"website_count":{"busuanzi_count":{"enable":false,"site_uv":false,"site_pv":false,"page_pv":false}},"version":"3.8.6"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
<div class="progress-bar-container">
    

    
</div>


<main class="page-container border-box">

    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.svg">
                </a>
            
            <a class="site-name border-box" href="/">
               Keep Theme
            </a>
        </div>

        <div class="right border-box">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                    
                </ul>
            </div>
            <div class="mobile">
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">

                

                    <div class="fade-in-down-animation">
    <div class="post-page-container border-box">

        <div class="article-content-container border-box">

            

            <div class="article-content-bottom border-box">
                
                    <div class="article-title">
                        
                    </div>
                

                
                    <div class="article-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.svg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author">
                                <span class="name">Keep Team</span>
                                
                                    <span class="author-label">Lv3</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="article-meta-info-container border-box post">
    <div class="article-meta-info border-box">
        


        
            <span class="meta-info-item article-create-date">
                <i class="icon fa-solid fa-calendar-check"></i>&nbsp;
                <span class="pc">2024-10-27 00:42:49</span>
                <span class="mobile">2024-10-27 00:42</span>
            </span>

            <span class="meta-info-item article-update-date">
                <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                <span class="pc" data-updated="Sun Oct 27 2024 00:42:49 GMT+0800">2024-10-27 00:42:49</span>
            </span>
        

        

        

        
        
        
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="article-content keep-markdown-body">
                    

                    <h2 id="lt-lt-lt-lt-lt-lt-lt-HEAD"><a href="#lt-lt-lt-lt-lt-lt-lt-HEAD" class="headerlink" title="&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD"></a>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</h2><p>title: 张量的广播和科学计算<br>author: xmcchv<br>date: 2021-07-27 21:46:44<br>mathjax: true<br>tags:</p>
<ul>
<li>pytorch</li>
</ul>
<hr>
<h2 id="张量的广播和科学运算"><a href="#张量的广播和科学运算" class="headerlink" title="张量的广播和科学运算"></a>张量的广播和科学运算</h2><p>作为pytorch中执行深度学习的基本数据类型，张量也拥有非常多的数学与运算函数和方法，以及对应的一系列计算规则。在pytorch中，能够作用于tensor的运算被统一称作为算子</p>
<ul>
<li>数学运算的分类</li>
</ul>
<ol>
<li>逐点运算（pointwise ops）对tensor中每个元素执行相同的运算操作</li>
<li>规约运算（reduction ops）对某一张两进行操作得出某种总结值</li>
<li>比较运算（comparison ops）对多个张量进行比较运算的相关方法</li>
<li>谱运算（spectral ops）涉及信号处理傅里叶变化的操作</li>
<li>BLAS和LAPACK运算 基础线性代数程序集（Basic Linear Algeria Subprograms）和线性代数包（Linear Algeria Package）中定义的、主要用于现行代数科学计算的函数和方法</li>
<li>其他运算 其他未被归类的数学运算</li>
</ol>
<h3 id="一、张量的广播-broadcast-特性"><a href="#一、张量的广播-broadcast-特性" class="headerlink" title="一、张量的广播(broadcast)特性"></a>一、张量的广播(broadcast)特性</h3><p>允许不同形状的张量之间进行计算</p>
<h4 id="1-相同形状的张量计算"><a href="#1-相同形状的张量计算" class="headerlink" title="1.相同形状的张量计算"></a>1.相同形状的张量计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">3</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1+t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1**t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 1, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1/t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([nan, 1., 1.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]+[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>[0, 1, 2, 0, 1, 2]
</code></pre>
<h4 id="2-不同形状的张量计算"><a href="#2-不同形状的张量计算" class="headerlink" title="2. 不同形状的张量计算"></a>2. 不同形状的张量计算</h4><p>广播的特性是在不同形状的张量进行计算时，一个或多个张量通过隐式转化为相同形状的两个张量，但是，并非任何两个不同形状的张量都可以通过广播特性进行计算</p>
<h5 id="2-1-标量和任意形状的张量"><a href="#2-1-标量和任意形状的张量" class="headerlink" title="2.1 标量和任意形状的张量"></a>2.1 标量和任意形状的张量</h5><p>标量可以和任意形状的张量进行计算，计算过程就是标量和张量的每一个元素进行计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1+<span class="number">1</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*<span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(t1+<span class="number">1</span>)**<span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 4, 9])
</code></pre>
<h5 id="2-2-相同维度、不同形状的计算"><a href="#2-2-相同维度、不同形状的计算" class="headerlink" title="2.2 相同维度、不同形状的计算"></a>2.2 相同维度、不同形状的计算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.zeros(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t21=torch.ones(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t21</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t21+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre>
<p>此处的广播相当于将t21的形状（1,4）拓展成了t2（3,4）<br>即复制了第一行三次，然后二者相加<br>也可理解为 t21的第一行和t2的三行分别进行相加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t22=torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t22</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t22.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<p>广播规则：</p>
<ol>
<li>两个张量在一个维度一致，另一个维度上其中一个张量为1</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.ones(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 1])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre>
<ol start="2">
<li>t3的形状为（3,1），t31的形状为（1,3），二者的形状在两个分量上均不同，但都有1存在，因此也可以广播</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t31=torch.ones(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t31+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])
</code></pre>
<p>三维张量的广播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t31=torch.ones(<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1.],
         [1.],
         [1.],
         [1.]],

        [[1.],
         [1.],
         [1.],
         [1.]],

        [[1.],
         [1.],
         [1.],
         [1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3+t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t32=torch.ones(<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">t32</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t32+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t32+t31 <span class="comment"># 一维度一致，另两个维度不同，但都有 1</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]],

        [[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]],

        [[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]]])
</code></pre>
<h5 id="2-3不同维度的张量计算过程中的广播"><a href="#2-3不同维度的张量计算过程中的广播" class="headerlink" title="2.3不同维度的张量计算过程中的广播"></a>2.3不同维度的张量计算过程中的广播</h5><p>低维张量升维，只需要将更高维度上填充1即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">4</span>).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1],
        [2, 3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0, 1],
         [2, 3]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[0, 1],
          [2, 3]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t3+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.],
        [1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t3+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
</code></pre>
<h3 id="二、逐点运算"><a href="#二、逐点运算" class="headerlink" title="二、逐点运算"></a>二、逐点运算</h3><h4 id="tensor基本数学运算"><a href="#tensor基本数学运算" class="headerlink" title="tensor基本数学运算"></a>tensor基本数学运算</h4><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.add(t1,t2)</td>
<td align="left">t1+t2</td>
</tr>
<tr>
<td align="left">torch.subtract(t1,t2)</td>
<td align="left">t1-t2</td>
</tr>
<tr>
<td align="left">torch.multiply(t1,t2)</td>
<td align="left">t1*t2</td>
</tr>
<tr>
<td align="left">torch.divide(t1,t2)</td>
<td align="left">t1&#x2F;t2</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([4, 6])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.multiply(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([3, 8])
</code></pre>
<h4 id="tensor数值调整函数"><a href="#tensor数值调整函数" class="headerlink" title="tensor数值调整函数"></a>tensor数值调整函数</h4><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.abs()</td>
<td align="left">返回绝对值</td>
</tr>
<tr>
<td align="left">torch.ceil()</td>
<td align="left">向上取整</td>
</tr>
<tr>
<td align="left">torch.floor()</td>
<td align="left">向下取整</td>
</tr>
<tr>
<td align="left">torch.round()</td>
<td align="left">四舍五入</td>
</tr>
<tr>
<td align="left">torch.neg()</td>
<td align="left">取反</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">5</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0.4222, -2.3513, -2.0652, -0.3838, -0.2116])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">round</span>(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0., -2., -2., -0., -0.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.abs_()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.4222, 2.3513, 2.0652, 0.3838, 0.2116])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.neg_()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0.4222, -2.3513, -2.0652, -0.3838, -0.2116])
</code></pre>
<table>
<thead>
<tr>
<th align="left">数学运算函数</th>
<th align="left">数学公式</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">幂运算</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">torch.exp(t)</td>
<td align="left">$ y_{i} &#x3D; e^{x_{i}} $</td>
<td align="left">返回以e为底、t中元素为幂的张量</td>
</tr>
<tr>
<td align="left">torch.expm1(t)</td>
<td align="left">$ y_{i} &#x3D; e^{x_{i}} $ - 1</td>
<td align="left">对张量中的所有元素计算exp（x） - 1</td>
</tr>
<tr>
<td align="left">torch.exp2(t)</td>
<td align="left">$ y_{i} &#x3D; 2^{x_{i}} $</td>
<td align="left">逐个元素计算2的t次方。</td>
</tr>
<tr>
<td align="left">torch.pow(t,n)</td>
<td align="left">$\text{out}_i &#x3D; x_i ^ \text{exponent} $</td>
<td align="left">返回t的n次幂</td>
</tr>
<tr>
<td align="left">torch.sqrt(t)</td>
<td align="left">$ \text{out}{i} &#x3D; \sqrt{\text{input}{i}} $</td>
<td align="left">返回t的平方根</td>
</tr>
<tr>
<td align="left">torch.square(t)</td>
<td align="left">$ \text{out}_i &#x3D; x_i ^ \text{2} $</td>
<td align="left">返回输入的元素平方。</td>
</tr>
<tr>
<td align="left">对数运算</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">torch.log10(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{10} (x_{i}) $</td>
<td align="left">返回以10为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{e} (x_{i}) $</td>
<td align="left">返回以e为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log2(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{2} (x_{i}) $</td>
<td align="left">返回以2为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log1p(t)</td>
<td align="left">$ y_i &#x3D; \log_{e} (x_i $ + 1)</td>
<td align="left">返回一个加自然对数的输入数组。</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(torch.tensor(<span class="number">2</span>),<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(4)
</code></pre>
<ul>
<li>tensor的大多数科学运算具有一定的静态性<br>静态性就是对输入的张量类型有明确的要求，例如部分函数只能输入浮点型张量，而不能输入整形张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t.dtype</span><br></pre></td></tr></table></figure>




<pre><code>torch.int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.7183,  7.3891, 20.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=t.<span class="built_in">float</span>()</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 2., 3.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.7183,  7.3891, 20.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.expm1(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1.7183,  6.3891, 19.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(t,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 4, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(t,<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.4142, 1.7321])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(torch.log(t1))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 2., 3.])
</code></pre>
<ul>
<li>排序运算：sort<br>在pytorch中，sort函数将同时返回排序结果和对应的索引值的排列</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.tensor([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([1, 2, 3, 4]),
indices=tensor([0, 2, 1, 3]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t,descending=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([4, 3, 2, 1]),
indices=tensor([3, 1, 2, 0]))
</code></pre>
<h3 id="三、规约运算"><a href="#三、规约运算" class="headerlink" title="三、规约运算"></a>三、规约运算</h3><p>规约运算指针对某张量进行某种总结，最后得出一个具体总结值的函数。主要包含数据科学领域内的诸多统计分析函数，如均值、极值、方差、中位数函数等等。</p>
<table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.mean(t)</td>
<td align="left">返回张量均值</td>
</tr>
<tr>
<td align="left">torch.var(t)</td>
<td align="left">返回张量方差</td>
</tr>
<tr>
<td align="left">torch.std(t)</td>
<td align="left">返回张量标准差</td>
</tr>
<tr>
<td align="left">torch.var_mean(t)</td>
<td align="left">返回张量方差和均值</td>
</tr>
<tr>
<td align="left">torch.std_mean(t)</td>
<td align="left">返回张量标准差和均值</td>
</tr>
<tr>
<td align="left">torch.max(t)</td>
<td align="left">返回张量最大值</td>
</tr>
<tr>
<td align="left">torch.argmax(t)</td>
<td align="left">返回张量最大值索引</td>
</tr>
<tr>
<td align="left">torch.min(t)</td>
<td align="left">返回张量最小值</td>
</tr>
<tr>
<td align="left">torch.argmin(t)</td>
<td align="left">返回张量最小值索引</td>
</tr>
<tr>
<td align="left">torch.median(t)</td>
<td align="left">返回张量中位数</td>
</tr>
<tr>
<td align="left">torch.sum(t)</td>
<td align="left">返回张量求和结果</td>
</tr>
<tr>
<td align="left">torch.logsumexp(t)</td>
<td align="left">返回张量各元素求和结果，适用于数据量较小的情况</td>
</tr>
<tr>
<td align="left">torch.prod(t)</td>
<td align="left">返回张量累乘结果</td>
</tr>
<tr>
<td align="left">torch.dist(t1, t2)</td>
<td align="left">计算两个张量的闵式距离，可使用不同范式</td>
</tr>
<tr>
<td align="left">torch.topk(t)</td>
<td align="left">返回t中最大的k个值对应的指标</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.std_mean(t)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor(3.0277), tensor(4.5000))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.prod(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>




<pre><code>tensor(24)
</code></pre>
<ul>
<li>dist计算距离<br>dist函数可计算闵式距离(闵可夫斯基距离)，通过输入不同的p值，可以计算多种类型的距离，如欧式距离，街道距离等。闵可夫斯基距离公式表示如下：</li>
</ul>
<p>$$D(x,y)&#x3D;(\sum_{u&#x3D;1}^{n}{|x_{u}-y_{u}|}^{p} )^{1&#x2F;p}$$</p>
<p>p取值为2时，计算欧式距离</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>])</span><br><span class="line">t2=torch.tensor([<span class="number">3.0</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dist(t1,t2,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2.8284)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sqrt(torch.tensor(<span class="number">8.0</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2.8284)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dist(t1,t2,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(4.)
</code></pre>
<ul>
<li>规约运算的维度<br>由于规约运算是一个序列返回一个结果，因此若是针对高维张良，则可能指定某维度进行计算。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).<span class="built_in">float</span>().reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t2,dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([12., 15., 18., 21.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t2,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 6., 22., 38.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">24</span>).<span class="built_in">float</span>().reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]],

        [[12., 13., 14., 15.],
         [16., 17., 18., 19.],
         [20., 21., 22., 23.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[12., 14., 16., 18.],
        [20., 22., 24., 26.],
        [28., 30., 32., 34.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[12., 15., 18., 21.],
        [48., 51., 54., 57.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 6., 22., 38.],
        [54., 70., 86.]])
</code></pre>
<ul>
<li>二维张量的排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t22=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t22</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.1265,  0.2807,  0.7180,  0.8772],
        [ 1.9022,  0.3509, -0.4520, -2.3835],
        [-0.5028, -0.8871, -2.2325,  0.7211]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t22) <span class="comment"># 默认情况下，按照行进行升序排序</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([[-0.1265,  0.2807,  0.7180,  0.8772],
        [-2.3835, -0.4520,  0.3509,  1.9022],
        [-2.2325, -0.8871, -0.5028,  0.7211]]),
indices=tensor([[0, 1, 2, 3],
        [3, 2, 1, 0],
        [2, 1, 0, 3]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t22,dim=<span class="number">1</span>,descending=<span class="literal">True</span>) <span class="comment"># 按列降序排序</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([[ 0.8772,  0.7180,  0.2807, -0.1265],
        [ 1.9022,  0.3509, -0.4520, -2.3835],
        [ 0.7211, -0.5028, -0.8871, -2.2325]]),
indices=tensor([[3, 2, 1, 0],
        [0, 1, 2, 3],
        [3, 0, 1, 2]]))
</code></pre>
<h3 id="四、比较运算"><a href="#四、比较运算" class="headerlink" title="四、比较运算"></a>四、比较运算</h3><p>比较运算是一类较为简单的运算类型，和python原生的布尔运算类似，常用于不同张量之间的逻辑运算，最终返回逻辑运算结果（逻辑类型张量）基本比较运算函数如下所示：</p>
<table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.eq(t1,t2)</td>
<td align="left">等效&#x3D;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.equal(t1,t2)</td>
<td align="left">判断两个张量是否是相同的张量</td>
</tr>
<tr>
<td align="left">torch.gt(t1,t2)</td>
<td align="left">等效&gt;</td>
</tr>
<tr>
<td align="left">torch.lt(t1,t2)</td>
<td align="left">等效于&lt;</td>
</tr>
<tr>
<td align="left">torch.ge(t1,t2)</td>
<td align="left">等效&gt;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.le(t1,t2)</td>
<td align="left">等效&lt;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.ne(t1,t2)</td>
<td align="left">等效!&#x3D;</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1.0</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">t2=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">t1==t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.equal(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>False
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.lt(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([False, False,  True])
</code></pre>
<h2 id="x3D-x3D-x3D-x3D-x3D-x3D-x3D"><a href="#x3D-x3D-x3D-x3D-x3D-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</h2><p>title: 张量的广播和科学计算<br>author: xmcchv<br>date: 2021-07-27 21:46:44<br>mathjax: true<br>tags:</p>
<ul>
<li>pytorch</li>
</ul>
<hr>
<h2 id="张量的广播和科学运算-1"><a href="#张量的广播和科学运算-1" class="headerlink" title="张量的广播和科学运算"></a>张量的广播和科学运算</h2><p>作为pytorch中执行深度学习的基本数据类型，张量也拥有非常多的数学与运算函数和方法，以及对应的一系列计算规则。在pytorch中，能够作用于tensor的运算被统一称作为算子</p>
<ul>
<li>数学运算的分类</li>
</ul>
<ol>
<li>逐点运算（pointwise ops）对tensor中每个元素执行相同的运算操作</li>
<li>规约运算（reduction ops）对某一张两进行操作得出某种总结值</li>
<li>比较运算（comparison ops）对多个张量进行比较运算的相关方法</li>
<li>谱运算（spectral ops）涉及信号处理傅里叶变化的操作</li>
<li>BLAS和LAPACK运算 基础线性代数程序集（Basic Linear Algeria Subprograms）和线性代数包（Linear Algeria Package）中定义的、主要用于现行代数科学计算的函数和方法</li>
<li>其他运算 其他未被归类的数学运算</li>
</ol>
<h3 id="一、张量的广播-broadcast-特性-1"><a href="#一、张量的广播-broadcast-特性-1" class="headerlink" title="一、张量的广播(broadcast)特性"></a>一、张量的广播(broadcast)特性</h3><p>允许不同形状的张量之间进行计算</p>
<h4 id="1-相同形状的张量计算-1"><a href="#1-相同形状的张量计算-1" class="headerlink" title="1.相同形状的张量计算"></a>1.相同形状的张量计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.arange(<span class="number">3</span>)</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1+t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 1, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1**t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 1, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1/t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([nan, 1., 1.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]+[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>




<pre><code>[0, 1, 2, 0, 1, 2]
</code></pre>
<h4 id="2-不同形状的张量计算-1"><a href="#2-不同形状的张量计算-1" class="headerlink" title="2. 不同形状的张量计算"></a>2. 不同形状的张量计算</h4><p>广播的特性是在不同形状的张量进行计算时，一个或多个张量通过隐式转化为相同形状的两个张量，但是，并非任何两个不同形状的张量都可以通过广播特性进行计算</p>
<h5 id="2-1-标量和任意形状的张量-1"><a href="#2-1-标量和任意形状的张量-1" class="headerlink" title="2.1 标量和任意形状的张量"></a>2.1 标量和任意形状的张量</h5><p>标量可以和任意形状的张量进行计算，计算过程就是标量和张量的每一个元素进行计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1+<span class="number">1</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1*<span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([0, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(t1+<span class="number">1</span>)**<span class="number">2</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 4, 9])
</code></pre>
<h5 id="2-2-相同维度、不同形状的计算-1"><a href="#2-2-相同维度、不同形状的计算-1" class="headerlink" title="2.2 相同维度、不同形状的计算"></a>2.2 相同维度、不同形状的计算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.zeros(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t21=torch.ones(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t21</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t21+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre>
<p>此处的广播相当于将t21的形状（1,4）拓展成了t2（3,4）<br>即复制了第一行三次，然后二者相加<br>也可理解为 t21的第一行和t2的三行分别进行相加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t22=torch.arange(<span class="number">9</span>).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">t22</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t22.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<p>广播规则：</p>
<ol>
<li>两个张量在一个维度一致，另一个维度上其中一个张量为1</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.ones(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">t3.size()</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 1])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
</code></pre>
<ol start="2">
<li>t3的形状为（3,1），t31的形状为（1,3），二者的形状在两个分量上均不同，但都有1存在，因此也可以广播</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t31=torch.ones(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1., 1., 1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t31+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])
</code></pre>
<p>三维张量的广播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]],

        [[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t31=torch.ones(<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1.],
         [1.],
         [1.],
         [1.]],

        [[1.],
         [1.],
         [1.],
         [1.]],

        [[1.],
         [1.],
         [1.],
         [1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3+t31</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t32=torch.ones(<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">t32</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t32+t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]],

        [[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t32+t31 <span class="comment"># 一维度一致，另两个维度不同，但都有 1</span></span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]],

        [[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]],

        [[2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.],
         [2., 2., 2., 2., 2.]]])
</code></pre>
<h5 id="2-3不同维度的张量计算过程中的广播-1"><a href="#2-3不同维度的张量计算过程中的广播-1" class="headerlink" title="2.3不同维度的张量计算过程中的广播"></a>2.3不同维度的张量计算过程中的广播</h5><p>低维张量升维，只需要将更高维度上填充1即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">4</span>).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 1],
        [2, 3]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0, 1],
         [2, 3]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[[0, 1],
          [2, 3]]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">t3+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]],

        [[0., 1.],
         [2., 3.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[1.],
        [1.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.zeros(<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t3+t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
</code></pre>
<h3 id="二、逐点运算-1"><a href="#二、逐点运算-1" class="headerlink" title="二、逐点运算"></a>二、逐点运算</h3><h4 id="tensor基本数学运算-1"><a href="#tensor基本数学运算-1" class="headerlink" title="tensor基本数学运算"></a>tensor基本数学运算</h4><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.add(t1,t2)</td>
<td align="left">t1+t2</td>
</tr>
<tr>
<td align="left">torch.subtract(t1,t2)</td>
<td align="left">t1-t2</td>
</tr>
<tr>
<td align="left">torch.multiply(t1,t2)</td>
<td align="left">t1*t2</td>
</tr>
<tr>
<td align="left">torch.divide(t1,t2)</td>
<td align="left">t1&#x2F;t2</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.tensor([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.add(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([4, 6])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.multiply(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([3, 8])
</code></pre>
<h4 id="tensor数值调整函数-1"><a href="#tensor数值调整函数-1" class="headerlink" title="tensor数值调整函数"></a>tensor数值调整函数</h4><table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.abs()</td>
<td align="left">返回绝对值</td>
</tr>
<tr>
<td align="left">torch.ceil()</td>
<td align="left">向上取整</td>
</tr>
<tr>
<td align="left">torch.floor()</td>
<td align="left">向下取整</td>
</tr>
<tr>
<td align="left">torch.round()</td>
<td align="left">四舍五入</td>
</tr>
<tr>
<td align="left">torch.neg()</td>
<td align="left">取反</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.randn(<span class="number">5</span>)</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0.4222, -2.3513, -2.0652, -0.3838, -0.2116])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">round</span>(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0., -2., -2., -0., -0.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.abs_()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0.4222, 2.3513, 2.0652, 0.3838, 0.2116])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.neg_()</span><br></pre></td></tr></table></figure>




<pre><code>tensor([-0.4222, -2.3513, -2.0652, -0.3838, -0.2116])
</code></pre>
<table>
<thead>
<tr>
<th align="left">数学运算函数</th>
<th align="left">数学公式</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">幂运算</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">torch.exp(t)</td>
<td align="left">$ y_{i} &#x3D; e^{x_{i}} $</td>
<td align="left">返回以e为底、t中元素为幂的张量</td>
</tr>
<tr>
<td align="left">torch.expm1(t)</td>
<td align="left">$ y_{i} &#x3D; e^{x_{i}} $ - 1</td>
<td align="left">对张量中的所有元素计算exp（x） - 1</td>
</tr>
<tr>
<td align="left">torch.exp2(t)</td>
<td align="left">$ y_{i} &#x3D; 2^{x_{i}} $</td>
<td align="left">逐个元素计算2的t次方。</td>
</tr>
<tr>
<td align="left">torch.pow(t,n)</td>
<td align="left">$\text{out}_i &#x3D; x_i ^ \text{exponent} $</td>
<td align="left">返回t的n次幂</td>
</tr>
<tr>
<td align="left">torch.sqrt(t)</td>
<td align="left">$ \text{out}{i} &#x3D; \sqrt{\text{input}{i}} $</td>
<td align="left">返回t的平方根</td>
</tr>
<tr>
<td align="left">torch.square(t)</td>
<td align="left">$ \text{out}_i &#x3D; x_i ^ \text{2} $</td>
<td align="left">返回输入的元素平方。</td>
</tr>
<tr>
<td align="left">对数运算</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">torch.log10(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{10} (x_{i}) $</td>
<td align="left">返回以10为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{e} (x_{i}) $</td>
<td align="left">返回以e为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log2(t)</td>
<td align="left">$ y_{i} &#x3D; \log_{2} (x_{i}) $</td>
<td align="left">返回以2为底的t的对数</td>
</tr>
<tr>
<td align="left">torch.log1p(t)</td>
<td align="left">$ y_i &#x3D; \log_{e} (x_i $ + 1)</td>
<td align="left">返回一个加自然对数的输入数组。</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(torch.tensor(<span class="number">2</span>),<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(4)
</code></pre>
<ul>
<li>tensor的大多数科学运算具有一定的静态性<br>静态性就是对输入的张量类型有明确的要求，例如部分函数只能输入浮点型张量，而不能输入整形张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">t.dtype</span><br></pre></td></tr></table></figure>




<pre><code>torch.int64
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 2, 3])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(t)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.7183,  7.3891, 20.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=t.<span class="built_in">float</span>()</span><br><span class="line">t1</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 2., 3.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 2.7183,  7.3891, 20.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.expm1(t1)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 1.7183,  6.3891, 19.0855])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(t,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 4, 9])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">pow</span>(t,<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1.0000, 1.4142, 1.7321])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(torch.log(t1))</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1., 2., 3.])
</code></pre>
<ul>
<li>排序运算：sort<br>在pytorch中，sort函数将同时返回排序结果和对应的索引值的排列</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.tensor([<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([1, 3, 2, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([1, 2, 3, 4]),
indices=tensor([0, 2, 1, 3]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t,descending=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([4, 3, 2, 1]),
indices=tensor([3, 1, 2, 0]))
</code></pre>
<h3 id="三、规约运算-1"><a href="#三、规约运算-1" class="headerlink" title="三、规约运算"></a>三、规约运算</h3><p>规约运算指针对某张量进行某种总结，最后得出一个具体总结值的函数。主要包含数据科学领域内的诸多统计分析函数，如均值、极值、方差、中位数函数等等。</p>
<table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.mean(t)</td>
<td align="left">返回张量均值</td>
</tr>
<tr>
<td align="left">torch.var(t)</td>
<td align="left">返回张量方差</td>
</tr>
<tr>
<td align="left">torch.std(t)</td>
<td align="left">返回张量标准差</td>
</tr>
<tr>
<td align="left">torch.var_mean(t)</td>
<td align="left">返回张量方差和均值</td>
</tr>
<tr>
<td align="left">torch.std_mean(t)</td>
<td align="left">返回张量标准差和均值</td>
</tr>
<tr>
<td align="left">torch.max(t)</td>
<td align="left">返回张量最大值</td>
</tr>
<tr>
<td align="left">torch.argmax(t)</td>
<td align="left">返回张量最大值索引</td>
</tr>
<tr>
<td align="left">torch.min(t)</td>
<td align="left">返回张量最小值</td>
</tr>
<tr>
<td align="left">torch.argmin(t)</td>
<td align="left">返回张量最小值索引</td>
</tr>
<tr>
<td align="left">torch.median(t)</td>
<td align="left">返回张量中位数</td>
</tr>
<tr>
<td align="left">torch.sum(t)</td>
<td align="left">返回张量求和结果</td>
</tr>
<tr>
<td align="left">torch.logsumexp(t)</td>
<td align="left">返回张量各元素求和结果，适用于数据量较小的情况</td>
</tr>
<tr>
<td align="left">torch.prod(t)</td>
<td align="left">返回张量累乘结果</td>
</tr>
<tr>
<td align="left">torch.dist(t1, t2)</td>
<td align="left">计算两个张量的闵式距离，可使用不同范式</td>
</tr>
<tr>
<td align="left">torch.topk(t)</td>
<td align="left">返回t中最大的k个值对应的指标</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t=torch.arange(<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">t</span><br></pre></td></tr></table></figure>




<pre><code>tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.std_mean(t)</span><br></pre></td></tr></table></figure>




<pre><code>(tensor(3.0277), tensor(4.5000))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.prod(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]))</span><br></pre></td></tr></table></figure>




<pre><code>tensor(24)
</code></pre>
<ul>
<li>dist计算距离<br>dist函数可计算闵式距离(闵可夫斯基距离)，通过输入不同的p值，可以计算多种类型的距离，如欧式距离，街道距离等。闵可夫斯基距离公式表示如下：</li>
</ul>
<p>$$D(x,y)&#x3D;(\sum_{u&#x3D;1}^{n}{|x_{u}-y_{u}|}^{p} )^{1&#x2F;p}$$</p>
<p>p取值为2时，计算欧式距离</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>])</span><br><span class="line">t2=torch.tensor([<span class="number">3.0</span>,<span class="number">4</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dist(t1,t2,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2.8284)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sqrt(torch.tensor(<span class="number">8.0</span>))</span><br></pre></td></tr></table></figure>




<pre><code>tensor(2.8284)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dist(t1,t2,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor(4.)
</code></pre>
<ul>
<li>规约运算的维度<br>由于规约运算是一个序列返回一个结果，因此若是针对高维张良，则可能指定某维度进行计算。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t2=torch.arange(<span class="number">12</span>).<span class="built_in">float</span>().reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t2.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t2,dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([12., 15., 18., 21.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t2,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ 6., 22., 38.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3=torch.arange(<span class="number">24</span>).<span class="built_in">float</span>().reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t3</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]],

        [[12., 13., 14., 15.],
         [16., 17., 18., 19.],
         [20., 21., 22., 23.]]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t3.shape</span><br></pre></td></tr></table></figure>




<pre><code>torch.Size([2, 3, 4])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[12., 14., 16., 18.],
        [20., 22., 24., 26.],
        [28., 30., 32., 34.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[12., 15., 18., 21.],
        [48., 51., 54., 57.]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(t3,dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[ 6., 22., 38.],
        [54., 70., 86.]])
</code></pre>
<ul>
<li>二维张量的排序</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t22=torch.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">t22</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.1265,  0.2807,  0.7180,  0.8772],
        [ 1.9022,  0.3509, -0.4520, -2.3835],
        [-0.5028, -0.8871, -2.2325,  0.7211]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t22) <span class="comment"># 默认情况下，按照行进行升序排序</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([[-0.1265,  0.2807,  0.7180,  0.8772],
        [-2.3835, -0.4520,  0.3509,  1.9022],
        [-2.2325, -0.8871, -0.5028,  0.7211]]),
indices=tensor([[0, 1, 2, 3],
        [3, 2, 1, 0],
        [2, 1, 0, 3]]))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(t22,dim=<span class="number">1</span>,descending=<span class="literal">True</span>) <span class="comment"># 按列降序排序</span></span><br></pre></td></tr></table></figure>




<pre><code>torch.return_types.sort(
values=tensor([[ 0.8772,  0.7180,  0.2807, -0.1265],
        [ 1.9022,  0.3509, -0.4520, -2.3835],
        [ 0.7211, -0.5028, -0.8871, -2.2325]]),
indices=tensor([[3, 2, 1, 0],
        [0, 1, 2, 3],
        [3, 0, 1, 2]]))
</code></pre>
<h3 id="四、比较运算-1"><a href="#四、比较运算-1" class="headerlink" title="四、比较运算"></a>四、比较运算</h3><p>比较运算是一类较为简单的运算类型，和python原生的布尔运算类似，常用于不同张量之间的逻辑运算，最终返回逻辑运算结果（逻辑类型张量）基本比较运算函数如下所示：</p>
<table>
<thead>
<tr>
<th align="left">函数</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">torch.eq(t1,t2)</td>
<td align="left">等效&#x3D;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.equal(t1,t2)</td>
<td align="left">判断两个张量是否是相同的张量</td>
</tr>
<tr>
<td align="left">torch.gt(t1,t2)</td>
<td align="left">等效&gt;</td>
</tr>
<tr>
<td align="left">torch.lt(t1,t2)</td>
<td align="left">等效于&lt;</td>
</tr>
<tr>
<td align="left">torch.ge(t1,t2)</td>
<td align="left">等效&gt;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.le(t1,t2)</td>
<td align="left">等效&lt;&#x3D;</td>
</tr>
<tr>
<td align="left">torch.ne(t1,t2)</td>
<td align="left">等效!&#x3D;</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t1=torch.tensor([<span class="number">1.0</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">t2=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">t1==t2</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.equal(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>False
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([ True, False, False])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.lt(t1,t2)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([False, False,  True])
</code></pre>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>285b814601d5399a58128ce8798b99dda8ed6d59</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

                </div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                    </div>
                    <div>
                        
                    </div>
                </div>

                

                
                    <div class="article-nav">
                        
                            <div class="article-prev">
                                <a class="prev"
                                   rel="prev"
                                   href="/2024/10/27/%E5%9F%BA%E6%9C%AC%E4%BC%98%E5%8C%96%E6%80%9D%E6%83%B3%E4%B8%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"
                                   title=""
                                >
                                    <span class="left arrow-icon flex-center">
                                      <i class="fas fa-chevron-left"></i>
                                    </span>
                                            <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis"></span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="article-next">
                                <a class="next"
                                   rel="next"
                                   href="/2024/10/27/%E5%BC%A0%E9%87%8F%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E8%BF%90%E7%AE%97/"
                                   title=""
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis"></span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                            <span class="right arrow-icon flex-center">
                                      <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="border-box website-info-box default">
        
            <div class="copyright-info info-item default">
                &copy;&nbsp;<span>2020</span>&nbsp;-&nbsp;2024
                
                    &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">Keep Team</a>
                
            </div>

            <div class="theme-info info-item default">
                由&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;驱动&nbsp;&&nbsp;主题&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
            </div>

            

            
        

        <div class="count-item info-item default">
            

            

            
        </div>
    </div>
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="tools-list border-box">
        <!-- PC TOC show toggle -->
        

        <!-- PC go comment -->
        
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    

    <!-- tablet toc -->
    
</main>



<!-- common -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/dark-light-toggle.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local-search -->


<!-- code-block -->


<!-- lazyload -->


<div class="">
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        

        <!-- copyright-info -->
        

        <!-- share -->
        
    

    <!-- category-page -->
    

    <!-- links-page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->



</body>
</html>
